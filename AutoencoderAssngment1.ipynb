# Autoencoder Assignment 1
# Name: [ray marange]
# Date: [04/11/2024] 
# Email: [rayc.marange@gmail.com]    
# Description: Implementation of an autoencoder with MSE objective, latent space control,
#              and generative capabilities, along with analysis of information-theoretic properties.
# Note: This code is structured to follow the assignment requirements step-by-step.
# University of VICTORIA WELLINGTON NEW ZEALAND

import numpy as np
import tensorflow as tf
layers = tf.keras.layers
models = tf.keras.models
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from scipy.stats import entropy, multivariate_normal
import seaborn as sns
from scipy.special import psi

###############################################################################
# 1. Create 3D data uniformly distributed over the surface of a cube
###############################################################################
print("=== 1. Generating 3D Cube Surface Data ===")

def generate_cube_surface_data(n_samples=10000):
    """
    Generate points uniformly distributed on the surface of a cube centered at origin with side length 2
    Returns:
        numpy array of shape (n_samples, 3) containing the 3D points
    """
    samples_per_face = n_samples // 6
    
    data = []
    # For each face of the cube (x=±1, y=±1, z=±1)
    for fixed_coord in [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0, -1), (0, 0, 1)]:
        if len(fixed_coord) == 2:  # x or y is fixed
            axis, value = fixed_coord
            if axis == -1:  # x = -1
                x = np.full(samples_per_face, -1)
                y = np.random.uniform(-1, 1, samples_per_face)
                z = np.random.uniform(-1, 1, samples_per_face)
            elif axis == 1:  # x = 1
                x = np.full(samples_per_face, 1)
                y = np.random.uniform(-1, 1, samples_per_face)
                z = np.random.uniform(-1, 1, samples_per_face)
            elif axis == 0 and value == -1:  # y = -1
                x = np.random.uniform(-1, 1, samples_per_face)
                y = np.full(samples_per_face, -1)
                z = np.random.uniform(-1, 1, samples_per_face)
            else:  # y = 1
                x = np.random.uniform(-1, 1, samples_per_face)
                y = np.full(samples_per_face, 1)
                z = np.random.uniform(-1, 1, samples_per_face)
        else:  # z is fixed
            _, _, value = fixed_coord
            if value == -1:  # z = -1
                x = np.random.uniform(-1, 1, samples_per_face)
                y = np.random.uniform(-1, 1, samples_per_face)
                z = np.full(samples_per_face, -1)
            else:  # z = 1
                x = np.random.uniform(-1, 1, samples_per_face)
                y = np.random.uniform(-1, 1, samples_per_face)
                z = np.full(samples_per_face, 1)
        
        face_data = np.column_stack((x, y, z))
        data.append(face_data)
    
    data = np.vstack(data)
    np.random.shuffle(data)
    return data

###############################################################################
# 2. Autoencoder with MSE objective and adjustable latent size
#    Enhanced with MMD regularization for latent space
###############################################################################
print("\n=== 2. Building Autoencoder with MSE Objective and MMD Regularization ===")

def compute_mmd(x, y, sigma=1.0):
    """
    Compute Maximum Mean Discrepancy (MMD) between two samples
    Uses Gaussian kernel with specified sigma
    """
    x = tf.convert_to_tensor(x, dtype=tf.float32)  # Ensure float32
    y = tf.convert_to_tensor(y, dtype=tf.float32)  # Ensure float32
    
    xx = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.square(x[:, None] - x[None, :]), axis=-1) / (2 * sigma**2)))
    yy = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.square(y[:, None] - y[None, :]), axis=-1) / (2 * sigma**2)))
    xy = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.square(x[:, None] - y[None, :]), axis=-1) / (2 * sigma**2)))
    
    return xx + yy - 2 * xy

def build_autoencoder(latent_dim=2, mmd_weight=0.1, mmd_sigma=1.0):
    """
    Build an autoencoder with adjustable latent dimension and MMD regularization
    Args:
        latent_dim: Dimension of the latent space
        mmd_weight: Weight for the MMD regularization term
        mmd_sigma: Sigma parameter for MMD Gaussian kernel
    Returns:
        autoencoder, encoder, decoder models
    """
    # Encoder
    encoder_input = layers.Input(shape=(3,))
    x = layers.Dense(64, activation='relu')(encoder_input)
    x = layers.Dense(32, activation='relu')(x)
    latent = layers.Dense(latent_dim, activation='linear', name='latent')(x)
    encoder_model = models.Model(encoder_input, latent)
    
    # Decoder
    latent_input = layers.Input(shape=(latent_dim,))
    x = layers.Dense(32, activation='relu')(latent_input)
    x = layers.Dense(64, activation='relu')(x)
    decoder_output = layers.Dense(3, activation='linear')(x)
    decoder_model = models.Model(latent_input, decoder_output)
    
    # Autoencoder
    autoencoder_input = layers.Input(shape=(3,))
    encoded = encoder_model(autoencoder_input)
    decoded = decoder_model(encoded)
    autoencoder_model = models.Model(autoencoder_input, decoded)
    
    # Custom training with MMD regularization
    optimizer = tf.keras.optimizers.Adam()
    loss_metric = tf.keras.metrics.Mean(name='loss')
    recon_loss_metric = tf.keras.metrics.Mean(name='reconstruction_loss')
    mmd_loss_metric = tf.keras.metrics.Mean(name='mmd_loss')
    
    @tf.function
    def train_step(data):
        with tf.GradientTape() as tape:
            # Forward pass
            latent = encoder_model(data, training=True)
            reconstructed = decoder_model(latent, training=True)
            
            # Reconstruction loss
            reconstruction_loss = tf.reduce_mean(tf.square(data - reconstructed))
            
            # MMD regularization - match latent distribution to standard normal
            true_samples = tf.random.normal(tf.shape(latent))
            mmd_loss = compute_mmd(latent, true_samples, sigma=mmd_sigma)
            
            # Total loss
            total_loss = reconstruction_loss + mmd_weight * mmd_loss
            
        # Compute gradients
        grads = tape.gradient(total_loss, 
                             encoder_model.trainable_variables + decoder_model.trainable_variables)
        optimizer.apply_gradients(
            zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))
        
        # Update metrics
        loss_metric.update_state(total_loss)
        recon_loss_metric.update_state(reconstruction_loss)
        mmd_loss_metric.update_state(mmd_loss)
        
        return {
            "loss": loss_metric.result(),
            "reconstruction_loss": recon_loss_metric.result(),
            "mmd_loss": mmd_loss_metric.result(),
        }
    
    # Create a custom model class to handle training
    class CustomAutoencoder(models.Model):
        def __init__(self, encoder, decoder, **kwargs):
            super(CustomAutoencoder, self).__init__(**kwargs)
            self.encoder = encoder
            self.decoder = decoder
            
        def call(self, inputs):
            latent = self.encoder(inputs)
            return self.decoder(latent)
            
        def train_step(self, data):
            return train_step(data)
            
        @property
        def metrics(self):
            return [loss_metric, recon_loss_metric, mmd_loss_metric]
    
    # Create and compile the model
    autoencoder = CustomAutoencoder(encoder_model, decoder_model)
    autoencoder.compile(optimizer=optimizer)
    
    return autoencoder, encoder_model, decoder_model

###############################################################################
# Multi-seed runs and ablation study
###############################################################################
print("\n=== Multi-seed Runs and Ablation Study ===")

# Define seeds for multi-seed runs
seeds = [42, 123, 456]
latent_dims = [1, 2, 4, 8, 16]
mmd_weight = 0.1

# Store results for each seed
all_results = {}

for seed in seeds:
    print(f"\n--- Training with seed {seed} ---")
    # Set random seeds for reproducibility
    np.random.seed(seed)
    tf.random.set_seed(seed)
    
    # Generate the data
    cube_data = generate_cube_surface_data(6000)
    cube_data = cube_data.astype(np.float32)
    print(f"Generated {cube_data.shape[0]} samples on cube surface")
    
    # Split into train and test
    train_data, test_data = train_test_split(cube_data, test_size=0.2, random_state=seed)
    
    autoencoders = {}
    histories = {}
    
    for dim in latent_dims:
        print(f"Training autoencoder with latent dimension {dim}")
        autoencoder, encoder, decoder = build_autoencoder(latent_dim=dim, mmd_weight=mmd_weight)
        
        # Custom training loop
        epochs = 100
        batch_size = 32
        n_batches = int(np.ceil(len(train_data) / batch_size))
        
        history = {'loss': [], 'reconstruction_loss': [], 'mmd_loss': [], 'val_loss': []}
        
        for epoch in range(epochs):
            # Shuffle training data
            indices = np.random.permutation(len(train_data))
            train_data_shuffled = train_data[indices]
            
            # Reset metrics
            for metric in autoencoder.metrics:
                metric.reset_state()
            
            # Train on batches
            for batch_idx in range(n_batches):
                batch_start = batch_idx * batch_size
                batch_end = min((batch_idx + 1) * batch_size, len(train_data))
                batch_data = train_data_shuffled[batch_start:batch_end]
                
                # Train step
                autoencoder.train_step(batch_data)
            
            # Record training metrics
            history['loss'].append(autoencoder.metrics[0].result().numpy())
            history['reconstruction_loss'].append(autoencoder.metrics[1].result().numpy())
            history['mmd_loss'].append(autoencoder.metrics[2].result().numpy())
            
            # Calculate validation loss
            val_reconstructions = autoencoder.predict(test_data, verbose=0)
            val_loss = np.mean(np.square(test_data - val_reconstructions))
            history['val_loss'].append(val_loss)
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}: loss={history['loss'][-1]:.4f}, val_loss={val_loss:.4f}")
        
        autoencoders[dim] = (autoencoder, encoder, decoder)
        histories[dim] = history
    
    # Store results for this seed
    all_results[seed] = {
        'autoencoders': autoencoders,
        'histories': histories,
        'train_data': train_data,
        'test_data': test_data
    }

# Calculate mean ± std across seeds for each latent dimension
print("\n=== Multi-seed Results (Mean ± Std) ===")
print("Dim\tFinal Val Loss\t\tReconstruction Loss\tMMD Loss")
for dim in latent_dims:
    val_losses = []
    recon_losses = []
    mmd_losses = []
    
    for seed in seeds:
        val_loss = all_results[seed]['histories'][dim]['val_loss'][-1]
        recon_loss = all_results[seed]['histories'][dim]['reconstruction_loss'][-1]
        mmd_loss = all_results[seed]['histories'][dim]['mmd_loss'][-1]
        
        val_losses.append(val_loss)
        recon_losses.append(recon_loss)
        mmd_losses.append(mmd_loss)
    
    print(f"{dim}\t{np.mean(val_losses):.6f} ± {np.std(val_losses):.6f}\t"
          f"{np.mean(recon_losses):.6f} ± {np.std(recon_losses):.6f}\t"
          f"{np.mean(mmd_losses):.6f} ± {np.std(mmd_losses):.6f}")

###############################################################################
# Ablation study: Effect of MMD sigma
###############################################################################
print("\n=== Ablation Study: Effect of MMD Sigma ===")

# Use a fixed seed for ablation study
ablation_seed = 42
np.random.seed(ablation_seed)
tf.random.set_seed(ablation_seed)

# Generate data
cube_data = generate_cube_surface_data(6000)
cube_data = cube_data.astype(np.float32)
train_data, test_data = train_test_split(cube_data, test_size=0.2, random_state=ablation_seed)

# Test different MMD sigma values
mmd_sigma_values = [0.1, 0.5, 1.0, 2.0, 5.0]
latent_dim = 2  # Fixed latent dimension for ablation study
mmd_weight = 0.1

sigma_results = {}

for sigma in mmd_sigma_values:
    print(f"\n--- Training with MMD sigma = {sigma} ---")
    autoencoder, encoder, decoder = build_autoencoder(latent_dim=latent_dim, mmd_weight=mmd_weight, mmd_sigma=sigma)
    
    # Custom training loop
    epochs = 100
    batch_size = 32
    n_batches = int(np.ceil(len(train_data) / batch_size))
    
    history = {'loss': [], 'reconstruction_loss': [], 'mmd_loss': [], 'val_loss': []}
    
    for epoch in range(epochs):
        # Shuffle training data
        indices = np.random.permutation(len(train_data))
        train_data_shuffled = train_data[indices]
        
        # Reset metrics
        for metric in autoencoder.metrics:
            metric.reset_state()
        
        # Train on batches
        for batch_idx in range(n_batches):
            batch_start = batch_idx * batch_size
            batch_end = min((batch_idx + 1) * batch_size, len(train_data))
            batch_data = train_data_shuffled[batch_start:batch_end]
            
            # Train step
            autoencoder.train_step(batch_data)
        
        # Record training metrics
        history['loss'].append(autoencoder.metrics[0].result().numpy())
        history['reconstruction_loss'].append(autoencoder.metrics[1].result().numpy())
        history['mmd_loss'].append(autoencoder.metrics[2].result().numpy())
        
        # Calculate validation loss
        val_reconstructions = autoencoder.predict(test_data, verbose=0)
        val_loss = np.mean(np.square(test_data - val_reconstructions))
        history['val_loss'].append(val_loss)
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch}: loss={history['loss'][-1]:.4f}, val_loss={val_loss:.4f}")
    
    # Evaluate generative performance
    def create_generative_system(encoder, decoder, data):
        """
        Create a generative system from the autoencoder
        Args:
            encoder: Encoder model
            decoder: Decoder model
            data: Training data to fit the latent distribution
        Returns:
            generate_samples: Function to generate new samples
            latent_mean: Mean of the latent distribution
            latent_cov: Covariance of the latent distribution
        """
        # Get latent representations of training data
        latent_codes = encoder.predict(data, verbose=0)
        
        # Fit a Gaussian distribution to the latent codes
        latent_mean = np.mean(latent_codes, axis=0)
        latent_cov = np.cov(latent_codes, rowvar=False)
        
        # Ensure covariance matrix is positive definite
        if len(latent_mean) > 1:
            # Add a small value to the diagonal to ensure positive definiteness
            latent_cov += np.eye(latent_cov.shape[0]) * 1e-6
        
        def generate_samples(n_samples):
            # Sample from the learned latent distribution
            if len(latent_mean) == 1:
                # 1D case
                latent_samples = np.random.normal(latent_mean[0], np.sqrt(latent_cov), (n_samples, 1))
            else:
                # Multi-dimensional case
                try:
                    latent_samples = np.random.multivariate_normal(latent_mean, latent_cov, n_samples)
                except:
                    # If covariance is not positive definite, use diagonal approximation
                    latent_cov_diag = np.diag(np.diag(latent_cov))
                    latent_samples = np.random.multivariate_normal(latent_mean, latent_cov_diag, n_samples)
            
            # Decode to generate new samples
            generated_samples = decoder.predict(latent_samples, verbose=0)
            return generated_samples

        return generate_samples, latent_mean, latent_cov

    def is_on_cube_surface(point, tolerance=0.1):
        """
        Check if a point is on the cube surface within tolerance
        Args:
            point: 3D point to check
            tolerance: Tolerance for being on the surface
        Returns:
            Boolean indicating if the point is on the cube surface
        """
        # A point is on the cube surface if at least one coordinate is ±1 within tolerance
        # and the other coordinates are within [-1, 1]
        on_surface = False
        for i in range(3):
            if abs(abs(point[i]) - 1) < tolerance:
                # Check if other coordinates are within bounds
                other_coords = [j for j in range(3) if j != i]
                if all(-1 <= point[j] <= 1 for j in other_coords):
                    on_surface = True
                    break
        return on_surface

    def kl_divergence(p, q):
        """
        Compute KL divergence between two distributions
        Args:
            p: First distribution (reference)
            q: Second distribution (approximation)
        Returns:
            KL divergence value
        """
        # Ensure distributions are normalized
        p = p / np.sum(p)
        q = q / np.sum(q)
        
        # Avoid zeros for KL calculation
        p = np.clip(p, 1e-10, 1)
        q = np.clip(q, 1e-10, 1)
        
        return np.sum(p * np.log(p / q))

    def evaluate_generative_performance(generate_samples, n_samples=1000, tolerance=0.1):
        """
        Evaluate the quality of generated samples
        Args:
            generate_samples: Function to generate samples
            n_samples: Number of samples to generate
            tolerance: Tolerance for surface detection
        Returns:
            surface_percentage: Percentage of samples on cube surface
            kl_divergence: KL divergence between original and generated distributions
            samples: Generated samples
        """
        # Generate samples
        samples = generate_samples(n_samples)
        
        # Calculate percentage of samples on cube surface
        on_surface_count = 0
        for sample in samples:
            if is_on_cube_surface(sample, tolerance):
                on_surface_count += 1
        
        surface_percentage = on_surface_count / n_samples * 100
        
        # Calculate distribution similarity using KL divergence
        # We'll compare the distribution of distances from origin and angles
        original_distances = np.linalg.norm(train_data, axis=1)
        generated_distances = np.linalg.norm(samples, axis=1)
        
        # Calculate histogram-based KL divergence for distances
        hist_original_dist, bin_edges_dist = np.histogram(original_distances, bins=50, density=True)
        hist_generated_dist, _ = np.histogram(generated_distances, bins=bin_edges_dist, density=True)
        
        kl_dist = kl_divergence(hist_original_dist, hist_generated_dist)
        
        # Calculate KL divergence for angular distribution (if 2D or 3D)
        if samples.shape[1] >= 2:
            # For 3D data, we can compute angles
            original_angles = np.arctan2(train_data[:, 1], train_data[:, 0])
            generated_angles = np.arctan2(samples[:, 1], samples[:, 0])
            
            hist_original_ang, bin_edges_ang = np.histogram(original_angles, bins=50, density=True)
            hist_generated_ang, _ = np.histogram(generated_angles, bins=bin_edges_ang, density=True)
            
            kl_ang = kl_divergence(hist_original_ang, hist_generated_ang)
            
            # Combine KL divergences
            total_kl = kl_dist + kl_ang
        else:
            total_kl = kl_dist

        return surface_percentage, total_kl, samples

    generate_samples, _, _ = create_generative_system(encoder, decoder, train_data)
    surface_percentage, kl_value, _ = evaluate_generative_performance(generate_samples)
    
    sigma_results[sigma] = {
        'final_val_loss': history['val_loss'][-1],
        'surface_percentage': surface_percentage,
        'kl_divergence': kl_value
    }

# Plot results for MMD sigma ablation
plt.figure(figsize=(15, 5))

# Final validation loss
plt.subplot(1, 3, 1)
sigma_values = list(sigma_results.keys())
val_losses = [sigma_results[s]['final_val_loss'] for s in sigma_values]
plt.plot(sigma_values, val_losses, 'o-')
plt.xscale('log')
plt.xlabel('MMD Sigma')
plt.ylabel('Final Validation Loss')
plt.title('Effect of MMD Sigma on Validation Loss')

# Surface percentage
plt.subplot(1, 3, 2)
surface_percentages = [sigma_results[s]['surface_percentage'] for s in sigma_values]
plt.plot(sigma_values, surface_percentages, 'o-')
plt.xscale('log')
plt.xlabel('MMD Sigma')
plt.ylabel('Surface Percentage (%)')
plt.title('Effect of MMD Sigma on Surface Percentage')

# KL divergence
plt.subplot(1, 3, 3)
kl_values = [sigma_results[s]['kl_divergence'] for s in sigma_values]
plt.plot(sigma_values, kl_values, 'o-')
plt.xscale('log')
plt.xlabel('MMD Sigma')
plt.ylabel('KL Divergence')
plt.title('Effect of MMD Sigma on KL Divergence')

plt.tight_layout()
plt.savefig('ablation_mmd_sigma.png')
plt.close()

# Print ablation results
print("\nMMD Sigma Ablation Results:")
print("Sigma\tVal Loss\tSurface %\tKL Divergence")
for sigma in mmd_sigma_values:
    result = sigma_results[sigma]
    print(f"{sigma}\t{result['final_val_loss']:.6f}\t{result['surface_percentage']:.2f}\t\t{result['kl_divergence']:.4f}")

###############################################################################
# Ablation study: Effect of surface tolerance
###############################################################################
print("\n=== Ablation Study: Effect of Surface Tolerance ===")

# Use the best model from the sigma ablation study
best_sigma = min(sigma_results, key=lambda k: sigma_results[k]['kl_divergence'])
print(f"Using best sigma value: {best_sigma}")

# Train a model with the best sigma
autoencoder, encoder, decoder = build_autoencoder(latent_dim=latent_dim, mmd_weight=mmd_weight, mmd_sigma=best_sigma)

# Custom training loop
epochs = 100
batch_size = 32
n_batches = int(np.ceil(len(train_data) / batch_size))

for epoch in range(epochs):
    # Shuffle training data
    indices = np.random.permutation(len(train_data))
    train_data_shuffled = train_data[indices]
    
    # Reset metrics
    for metric in autoencoder.metrics:
        metric.reset_state()
    
    # Train on batches
    for batch_idx in range(n_batches):
        batch_start = batch_idx * batch_size
        batch_end = min((batch_idx + 1) * batch_size, len(train_data))
        batch_data = train_data_shuffled[batch_start:batch_end]
        
        # Train step
        autoencoder.train_step(batch_data)
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch}: loss={autoencoder.metrics[0].result().numpy():.4f}")

# Test different surface tolerance values
tolerance_values = [0.01, 0.05, 0.1, 0.2, 0.3]
tolerance_results = {}

generate_samples, _, _ = create_generative_system(encoder, decoder, train_data)

for tolerance in tolerance_values:
    print(f"\n--- Evaluating with surface tolerance = {tolerance} ---")
    surface_percentage, kl_value, samples = evaluate_generative_performance(generate_samples, tolerance=tolerance)
    
    tolerance_results[tolerance] = {
        'surface_percentage': surface_percentage,
        'kl_divergence': kl_value
    }

# Plot results for surface tolerance ablation
plt.figure(figsize=(10, 5))

# Surface percentage
plt.subplot(1, 2, 1)
tolerance_values_list = list(tolerance_results.keys())
surface_percentages = [tolerance_results[t]['surface_percentage'] for t in tolerance_values_list]
plt.plot(tolerance_values_list, surface_percentages, 'o-')
plt.xlabel('Surface Tolerance')
plt.ylabel('Surface Percentage (%)')
plt.title('Effect of Surface Tolerance on Surface Percentage')

# KL divergence
plt.subplot(1, 2, 2)
kl_values = [tolerance_results[t]['kl_divergence'] for t in tolerance_values_list]
plt.plot(tolerance_values_list, kl_values, 'o-')
plt.xlabel('Surface Tolerance')
plt.ylabel('KL Divergence')
plt.title('Effect of Surface Tolerance on KL Divergence')

plt.tight_layout()
plt.savefig('ablation_surface_tolerance.png')
plt.close()

# Print tolerance ablation results
print("\nSurface Tolerance Ablation Results:")
print("Tolerance\tSurface %\tKL Divergence")
for tolerance in tolerance_values:
    result = tolerance_results[tolerance]
    print(f"{tolerance}\t\t{result['surface_percentage']:.2f}\t\t{result['kl_divergence']:.4f}")

###############################################################################
# 3. Probabilistic interpretation of MSE objective function
###############################################################################
print("\n=== 3. Probabilistic Interpretation of MSE ===")
print("The MSE objective function can be interpreted as maximizing the log-likelihood")
print("of the data under a Gaussian distribution with fixed variance.")
print("Specifically, minimizing MSE is equivalent to maximizing:")
print("log p(X|Z) = -1/(2σ²) * MSE(X, X̂) + constant")
print("where σ² is the fixed variance of the Gaussian noise model.")
print("This assumes that the reconstruction errors are normally distributed with mean 0.")
print("This corresponds to Maximum Likelihood Estimation (MLE) under the Gaussian assumption.")

###############################################################################
# 4. Method to control latent distribution and add noise
###############################################################################
print("\n=== 4. Controlling Latent Distribution with Noise ===")

def add_latent_noise(encoder, data, snr_db):
    """
    Add Gaussian noise to latent representation with specified SNR
    Returns noisy latent codes and the information rate
    Args:
        encoder: Encoder model
        data: Input data
        snr_db: Signal-to-noise ratio in dB
    Returns:
        noisy_latent: Latent codes with added noise
        total_information: Information rate in bits/sample
        signal_power: Power of the signal
        noise_power: Power of the noise
    """
    # Get latent representation
    latent_codes = encoder.predict(data, verbose=0)
    
    # Calculate signal power (variance)
    signal_power = np.var(latent_codes, axis=0)
    
    # Convert SNR from dB to linear scale
    snr_linear = 10**(snr_db / 10)
    
    # Calculate noise power for each dimension
    noise_power = signal_power / snr_linear
    
    # Add Gaussian noise
    noise = np.random.normal(0, np.sqrt(noise_power), latent_codes.shape)
    noisy_latent = latent_codes + noise
    
    # Calculate information rate using equation (2)
    # I(Y;Z) = 0.5 * log2(σ_z² / σ_ε²) = 0.5 * log2(SNR) for each dimension
    information_rate = 0.5 * np.log2(1 + snr_linear)  # bits per dimension per sample
    
    total_information = np.sum(information_rate)
    
    return noisy_latent, total_information, signal_power, noise_power

# Test with different SNRs using the best model from the first seed
encoder = all_results[seeds[0]]['autoencoders'][2][1]  # Use dim=2 from first seed
test_data = all_results[seeds[0]]['test_data']

snr_values = [-10, -5, 0, 5, 10, 20, 30]  # in dB

print("\nInformation Rate at Different SNRs:")
print("SNR (dB)\tInfo Rate (bits/sample)\tSignal Power\tNoise Power")
for snr_db in snr_values:
    _, info_rate, signal_power, noise_power = add_latent_noise(encoder, test_data, snr_db)
    print(f"{snr_db}\t\t{info_rate:.4f}\t\t\t{np.mean(signal_power):.4f}\t\t{np.mean(noise_power):.4f}")

###############################################################################
# 5. Analyze reconstructions at various SNRs and latent dimensions
###############################################################################
print("\n=== 5. Analyzing Reconstructions at Various SNRs and Latent Dimensions ===")

def evaluate_reconstruction(encoder, decoder, data, snr_db):
    """
    Evaluate reconstruction quality with added noise
    Args:
        encoder: Encoder model
        decoder: Decoder model
        data: Input data
        snr_db: Signal-to-noise ratio in dB
    Returns:
        mse: Mean squared error of reconstruction
        info_rate: Information rate in bits/sample
        reconstructed: Reconstructed data
    """
    # Get noisy latent codes
    noisy_latent, info_rate, _, _ = add_latent_noise(encoder, data, snr_db)
    
    # Reconstruct
    reconstructed = decoder.predict(noisy_latent, verbose=0)
    
    # Calculate reconstruction error
    mse = np.mean(np.square(data - reconstructed))
    
    return mse, info_rate, reconstructed

# Evaluate for different latent dimensions and SNRs using the first seed
results = {}
autoencoders_seed0 = all_results[seeds[0]]['autoencoders']
test_data_seed0 = all_results[seeds[0]]['test_data']

print("\nReconstruction Quality Analysis:")
print("Dim\tSNR (dB)\tMSE\t\tInfo Rate (bits/sample)")
for dim in latent_dims:
    encoder = autoencoders_seed0[dim][1]
    decoder = autoencoders_seed0[dim][2]
    
    results[dim] = {}
    
    for snr_db in snr_values:
        mse, info_rate, reconstructed = evaluate_reconstruction(encoder, decoder, test_data_seed0, snr_db)
        results[dim][snr_db] = {'mse': mse, 'info_rate': info_rate}
        print(f"{dim}\t{snr_db}\t\t{mse:.6f}\t{info_rate:.4f}")

# Visualize reconstructions for a specific latent dimension and SNR
dim = 2
snr_db = 10
encoder = autoencoders_seed0[dim][1]
decoder = autoencoders_seed0[dim][2]

_, _, reconstructed = evaluate_reconstruction(encoder, decoder, test_data_seed0, snr_db)

# Plot original and reconstructed
fig = plt.figure(figsize=(15, 5))

# Original data
ax1 = fig.add_subplot(131, projection='3d')
sample_idx = np.random.choice(len(test_data_seed0), 500, replace=False)
ax1.scatter(test_data_seed0[sample_idx, 0], test_data_seed0[sample_idx, 1], test_data_seed0[sample_idx, 2], alpha=0.5)
ax1.set_title('Original Test Data')
ax1.set_xlim(-1.5, 1.5)
ax1.set_ylim(-1.5, 1.5)
ax1.set_zlim(-1.5, 1.5)

# Reconstructed data
ax2 = fig.add_subplot(132, projection='3d')
ax2.scatter(reconstructed[sample_idx, 0], reconstructed[sample_idx, 1], reconstructed[sample_idx, 2], alpha=0.5)
ax2.set_title(f'Reconstructed (Dim={dim}, SNR={snr_db}dB)')
ax2.set_xlim(-1.5, 1.5)
ax2.set_ylim(-1.5, 1.5)
ax2.set_zlim(-1.5, 1.5)

# Error visualization
errors = np.linalg.norm(test_data_seed0 - reconstructed, axis=1)
ax3 = fig.add_subplot(133)
ax3.hist(errors, bins=50)
ax3.set_title('Reconstruction Error Distribution')
ax3.set_xlabel('Euclidean Distance Error')
ax3.set_ylabel('Frequency')

plt.tight_layout()
plt.savefig('reconstruction_comparison.png')
plt.close()

###############################################################################
# 6. Create a generative system and define quality measures
###############################################################################
print("\n=== 6. Creating Generative System with Quality Measures ===")

# Evaluate generative performance for different latent dimensions using the first seed
print("\nGenerative Performance Evaluation:")
print("Dim\tSurface %\tKL Divergence")
generative_performance = {}

for dim in latent_dims:
    encoder = autoencoders_seed0[dim][1]
    decoder = autoencoders_seed0[dim][2]
    train_data_seed0 = all_results[seeds[0]]['train_data']
    
    generate_samples, latent_mean, latent_cov = create_generative_system(encoder, decoder, train_data_seed0)
    surface_percentage, kl_value, samples = evaluate_generative_performance(generate_samples)

    generative_performance[dim] = {
        'surface_percentage': surface_percentage,
        'kl_divergence': kl_value
    }
    print(f"{dim}\t{surface_percentage:.2f}%\t\t{kl_value:.4f}")

# Visualize generated samples for the best model
best_dim = min(generative_performance, key=lambda k: generative_performance[k]['kl_divergence'])
encoder = autoencoders_seed0[best_dim][1]
decoder = autoencoders_seed0[best_dim][2]
generate_samples, _, _ = create_generative_system(encoder, decoder, train_data_seed0)
_, _, generated_samples = evaluate_generative_performance(generate_samples, 1000)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(generated_samples[:, 0], generated_samples[:, 1], generated_samples[:, 2], alpha=0.5)
ax.set_title(f'Generated Samples (Best Model: Dim={best_dim})')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.set_xlim(-1.5, 1.5)
ax.set_ylim(-1.5, 1.5)
ax.set_zlim(-1.5, 1.5)
plt.savefig('generated_samples.png')
plt.close()

###############################################################################
# 7. Improved Mutual Information Estimation using k-NN method
###############################################################################
print("\n=== 7. Improved Mutual Information Estimation ===")

def knn_mi_estimation(x, y, k=5):
    """
    Estimate mutual information between x and y using k-NN method
    Based on Kraskov et al. (2004) method
    Args:
        x: First variable (n_samples, n_features_x)
        y: Second variable (n_samples, n_features_y)
        k: Number of nearest neighbors to use
    Returns:
        mi: Estimated mutual information in bits
    """
    n_samples = x.shape[0]
    
    # Combine x and y
    xy = np.hstack((x, y))
    
    # Find k-nearest neighbors in the joint space
    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(xy)
    distances, _ = nbrs.kneighbors(xy)
    
    # The k+1-th neighbor is the farthest (we exclude the point itself)
    epsilon = distances[:, -1]
    
    # Count neighbors in x and y spaces within epsilon
    n_x = np.zeros(n_samples)
    n_y = np.zeros(n_samples)
    
    for i in range(n_samples):
        # Count points in x-space within epsilon
        n_x[i] = np.sum(np.linalg.norm(x - x[i], axis=1) < epsilon[i])
        
        # Count points in y-space within epsilon
        n_y[i] = np.sum(np.linalg.norm(y - y[i], axis=1) < epsilon[i])
    
    # Estimate mutual information using Kraskov's formula
    psi_n = psi(n_samples)
    psi_k = psi(k)
    
    mi = psi_n + psi_k - np.mean(psi(n_x + 1) + psi(n_y + 1))
    
    # Convert from nats to bits
    mi_bits = mi / np.log(2)
    
    return max(0, mi_bits)

# Calculate mutual information between input and latent representations using the first seed
mi_results = {}
for dim in latent_dims:
    encoder = autoencoders_seed0[dim][1]
    latent_codes = encoder.predict(test_data_seed0, verbose=0)
    
    # For each dimension, calculate MI with input
    total_mi = 0
    for i in range(latent_codes.shape[1]):  # For each latent dimension
        latent_dim_i = latent_codes[:, i].reshape(-1, 1)
        mi = knn_mi_estimation(test_data_seed0, latent_dim_i, k=5)
        total_mi += mi
    
    mi_results[dim] = total_mi
    print(f"Latent Dimension: {dim}, Mutual Information: {total_mi:.4f} bits")

# Plot information vs reconstruction error
mi_values = [mi_results[dim] for dim in latent_dims]
mse_values = [all_results[seeds[0]]['histories'][dim]['val_loss'][-1] for dim in latent_dims]

plt.figure(figsize=(10, 6))
plt.plot(mi_values, mse_values, 'o-')
for i, dim in enumerate(latent_dims):
    plt.annotate(f'dim={dim}', (mi_values[i], mse_values[i]), xytext=(5, 5), textcoords='offset points')
plt.xlabel('Mutual Information (bits)')
plt.ylabel('Reconstruction Error (MSE)')
plt.title('Information-Reconstruction Trade-off')
plt.grid(True)
plt.savefig('information_reconstruction_tradeoff.png')
plt.close()

###############################################################################
# 8. Compare with VAE approach (theoretical comparison)
###############################################################################
print("\n=== 8. Comparison with VAE Approach ===")
print("Our noise-based approach controls information flow by adding Gaussian noise")
print("to the latent representation, which is similar to the VAE's approach of")
print("modeling the latent distribution as Gaussian.")
print()
print("Key differences:")
print("1. Our approach explicitly controls SNR, while VAE uses KL divergence")
print("2. VAE learns both mean and variance of the latent distribution")
print("3. Our MMD regularization encourages Gaussian latent distribution like VAE")
print("4. VAE's ELBO objective combines reconstruction and distribution matching")
print()
print("Both approaches aim to create a well-structured latent space suitable")
print("for generation, but VAE provides a more principled probabilistic framework.")

###############################################################################
# 10. Missing Visualization Plots
###############################################################################
print("\n=== 10. Generating Missing Visualization Plots ===")

# 1. 3D Visualization of Original Data vs Reconstructions at Various SNRs and Dimensions
def plot_3d_comparisons():
    """Create 3D plots comparing original data with reconstructions at various settings"""
    # Use the first seed for consistency
    seed_data = all_results[seeds[0]]
    
    # Select a subset of data for visualization
    sample_indices = np.random.choice(len(seed_data['test_data']), 500, replace=False)
    original_samples = seed_data['test_data'][sample_indices]
    
    # Create subplots for different dimensions and SNRs
    fig = plt.figure(figsize=(20, 15))
    
    # Settings to visualize
    dims_to_plot = [1, 2, 4, 8]
    snrs_to_plot = [-10, 0, 10, 30]
    
    plot_idx = 1
    for i, dim in enumerate(dims_to_plot):
        encoder = seed_data['autoencoders'][dim][1]
        decoder = seed_data['autoencoders'][dim][2]
        
        for j, snr_db in enumerate(snrs_to_plot):
            # Get reconstructions with noise
            _, _, reconstructed = evaluate_reconstruction(encoder, decoder, original_samples, snr_db)
            
            # Original data
            ax = fig.add_subplot(len(dims_to_plot), len(snrs_to_plot)*2, plot_idx, projection='3d')
            ax.scatter(original_samples[:, 0], original_samples[:, 1], original_samples[:, 2], 
                      alpha=0.5, s=10)
            ax.set_title(f'Original Data\n(Dim={dim}, SNR={snr_db}dB)')
            ax.set_xlim(-1.5, 1.5)
            ax.set_ylim(-1.5, 1.5)
            ax.set_zlim(-1.5, 1.5)
            plot_idx += 1
            
            # Reconstructed data
            ax = fig.add_subplot(len(dims_to_plot), len(snrs_to_plot)*2, plot_idx, projection='3d')
            ax.scatter(reconstructed[:, 0], reconstructed[:, 1], reconstructed[:, 2], 
                      alpha=0.5, s=10, c='red')
            ax.set_title(f'Reconstructed Data\n(Dim={dim}, SNR={snr_db}dB)')
            ax.set_xlim(-1.5, 1.5)
            ax.set_ylim(-1.5, 1.5)
            ax.set_zlim(-1.5, 1.5)
            plot_idx += 1
    
    plt.tight_layout()
    plt.savefig('3d_reconstruction_comparisons.png', dpi=150, bbox_inches='tight')
    plt.close()

# 2. Validation Loss Curves for Different Latent Dimensions
def plot_validation_curves():
    """Plot validation loss curves for different latent dimensions"""
    plt.figure(figsize=(12, 8))
    
    # Use the first seed for consistency
    seed_data = all_results[seeds[0]]
    
    for dim in latent_dims:
        val_loss = seed_data['histories'][dim]['val_loss']
        plt.plot(val_loss, label=f'Latent Dim = {dim}')
    
    plt.xlabel('Epoch')
    plt.ylabel('Validation Loss (MSE)')
    plt.title('Validation Loss Curves for Different Latent Dimensions')
    plt.legend()
    plt.grid(True)
    plt.yscale('log')  # Use log scale for better visualization
    plt.savefig('validation_loss_curves.png', dpi=150, bbox_inches='tight')
    plt.close()

# 3. Information Rate vs Reconstruction Quality
def plot_info_rate_vs_quality():
    """Plot information rate vs reconstruction quality for different dimensions"""
    plt.figure(figsize=(12, 8))
    
    # Use the first seed for consistency
    seed_data = all_results[seeds[0]]
    
    colors = plt.cm.viridis(np.linspace(0, 1, len(latent_dims)))
    
    for i, dim in enumerate(latent_dims):
        mse_values = []
        info_rates = []
        
        for snr_db in snr_values:
            mse = results[dim][snr_db]['mse']
            info_rate = results[dim][snr_db]['info_rate']
            mse_values.append(mse)
            info_rates.append(info_rate)
        
        plt.plot(info_rates, mse_values, 'o-', color=colors[i], label=f'Dim={dim}', markersize=6)
        
        # Annotate SNR points
        for j, snr_db in enumerate(snr_values):
            if snr_db in [-10, 0, 10, 20, 30]:  # Only label some points to avoid clutter
                plt.annotate(f'{snr_db}dB', 
                            (info_rates[j], mse_values[j]),
                            xytext=(5, 5), textcoords='offset points',
                            fontsize=8, alpha=0.7)
    
    plt.xlabel('Information Rate (bits/sample)')
    plt.ylabel('Reconstruction MSE')
    plt.title('Information Rate vs Reconstruction Quality')
    plt.legend()
    plt.grid(True)
    plt.yscale('log')
    plt.savefig('info_rate_vs_quality.png', dpi=150, bbox_inches='tight')
    plt.close()

# 4. Latent Space Visualization for Different Dimensions
def plot_latent_spaces():
    """Visualize latent spaces for different dimensions"""
    # Use the first seed for consistency
    seed_data = all_results[seeds[0]]
    
    # Create a grid of plots
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.ravel()
    
    for i, dim in enumerate(latent_dims[:6]):  # Only plot first 6 dimensions
        encoder = seed_data['autoencoders'][dim][1]
        latent_codes = encoder.predict(seed_data['test_data'], verbose=0)
        
        if dim == 1:
            # 1D histogram
            axes[i].hist(latent_codes.flatten(), bins=50, alpha=0.7)
            axes[i].set_title(f'Latent Dimension {dim}')
            axes[i].set_xlabel('Latent Value')
            axes[i].set_ylabel('Frequency')
        elif dim == 2:
            # 2D scatter plot
            axes[i].scatter(latent_codes[:, 0], latent_codes[:, 1], alpha=0.5, s=10)
            axes[i].set_title(f'Latent Dimension {dim}')
            axes[i].set_xlabel('Latent Dimension 1')
            axes[i].set_ylabel('Latent Dimension 2')
        else:
            # For higher dimensions, show the first two dimensions
            axes[i].scatter(latent_codes[:, 0], latent_codes[:, 1], alpha=0.5, s=10)
            axes[i].set_title(f'Latent Dimension {dim} (First 2 dims)')
            axes[i].set_xlabel('Latent Dimension 1')
            axes[i].set_ylabel('Latent Dimension 2')
    
    # Remove empty subplots
    for i in range(len(latent_dims), 6):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.savefig('latent_space_visualizations.png', dpi=150, bbox_inches='tight')
    plt.close()

# 5. Training History for Different Latent Dimensions
def plot_training_history():
    """Plot training history for different latent dimensions"""
    # Use the first seed for consistency
    seed_data = all_results[seeds[0]]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()
    
    metrics = ['loss', 'reconstruction_loss', 'mmd_loss', 'val_loss']
    titles = ['Total Loss', 'Reconstruction Loss', 'MMD Loss', 'Validation Loss']
    
    for i, metric in enumerate(metrics):
        for dim in latent_dims:
            history = seed_data['histories'][dim][metric]
            axes[i].plot(history, label=f'Dim={dim}')
        
        axes[i].set_title(titles[i])
        axes[i].set_xlabel('Epoch')
        axes[i].set_ylabel('Loss')
        axes[i].legend()
        axes[i].grid(True)
        if metric != 'mmd_loss':  # MMD loss is typically much smaller
            axes[i].set_yscale('log')
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    plt.close()

# Generate all the missing plots
print("Generating 3D comparison plots...")
plot_3d_comparisons()

print("Generating validation loss curves...")
plot_validation_curves()

print("Generating information rate vs quality plot...")
plot_info_rate_vs_quality()

print("Generating latent space visualizations...")
plot_latent_spaces()

print("Generating training history plots...")
plot_training_history()

print("All missing visualizations have been generated and saved as PNG files.")

###############################################################################
# 9. Summary of Results
###############################################################################
print("\n" + "="*50)
print("SUMMARY OF RESULTS")
print("="*50)

print("\n1. Autoencoder Performance by Latent Dimension (Mean ± Std across seeds):")
print("Dim\tFinal MSE\tMutual Info (bits)")
for dim in latent_dims:
    val_losses = [all_results[seed]['histories'][dim]['val_loss'][-1] for seed in seeds]
    final_mse = np.mean(val_losses)
    mi = mi_results[dim]
    print(f"{dim}\t{final_mse:.6f} ± {np.std(val_losses):.6f}\t{mi:.4f}")

print("\n2. Information Rate at Different SNRs (for dim=2):")
for snr_db in snr_values:
    _, info_rate, _, _ = add_latent_noise(encoder, test_data_seed0, snr_db)
    print(f"SNR: {snr_db} dB -> Info Rate: {info_rate:.4f} bits/sample")

print("\n3. Generative Performance:")
print("Dim\tSurface %\tKL Divergence")
for dim in latent_dims:
    perf = generative_performance[dim]
    print(f"{dim}\t{perf['surface_percentage']:.2f}%\t\t{perf['kl_divergence']:.4f}")

print("\n4. MMD Sigma Ablation Results:")
print("Sigma\tVal Loss\tSurface %\tKL Divergence")
for sigma in mmd_sigma_values:
    result = sigma_results[sigma]
    print(f"{sigma}\t{result['final_val_loss']:.6f}\t{result['surface_percentage']:.2f}\t\t{result['kl_divergence']:.4f}")

print("\n5. Surface Tolerance Ablation Results:")
print("Tolerance\tSurface %\tKL Divergence")
for tolerance in tolerance_values:
    result = tolerance_results[tolerance]
    print(f"{tolerance}\t\t{result['surface_percentage']:.2f}\t\t{result['kl_divergence']:.4f}")

print("\n6. Key Observations:")
print("- Higher latent dimensions allow more information preservation but may overfit")
print("- Very low SNRs (<0 dB) destroy most information and yield poor reconstructions")
print("- The optimal latent dimension for generation appears to be around 2-4")
print("- The autoencoder learns to capture the cube surface structure reasonably well")
print("- Information rate increases with SNR as expected from theory")
print("- MMD regularization helps structure the latent space for better generation")
print("- MMD sigma affects both reconstruction quality and generative performance")
print("- Surface tolerance parameter affects the evaluation of generative quality")

# Save results to file
with open('results_summary.txt', 'w') as f:
    f.write("RESULTS SUMMARY\n")
    f.write("===============\n\n")
    
    f.write("Autoencoder Performance by Latent Dimension (Mean ± Std across seeds):\n")
    f.write("Dim\tFinal MSE\tMutual Info (bits)\n")
    for dim in latent_dims:
        val_losses = [all_results[seed]['histories'][dim]['val_loss'][-1] for seed in seeds]
        final_mse = np.mean(val_losses)
        mi = mi_results[dim]
        f.write(f"{dim}\t{final_mse:.6f} ± {np.std(val_losses):.6f}\t{mi:.4f}\n")
    
    f.write("\nInformation Rate at Different SNRs (for dim=2):\n")
    for snr_db in snr_values:
        _, info_rate, _, _ = add_latent_noise(encoder, test_data_seed0, snr_db)
        f.write(f"SNR: {snr_db} dB -> Info Rate: {info_rate:.4f} bits/sample\n")
    
    f.write("\nGenerative Performance:\n")
    f.write("Dim\tSurface %\tKL Divergence\n")
    for dim in latent_dims:
        perf = generative_performance[dim]
        f.write(f"{dim}\t{perf['surface_percentage']:.2f}%\t\t{perf['kl_divergence']:.4f}\n")
    
    f.write("\nMMD Sigma Ablation Results:\n")
    f.write("Sigma\tVal Loss\tSurface %\tKL Divergence\n")
    for sigma in mmd_sigma_values:
        result = sigma_results[sigma]
        f.write(f"{sigma}\t{result['final_val_loss']:.6f}\t{result['surface_percentage']:.2f}\t\t{result['kl_divergence']:.4f}\n")
    
    f.write("\nSurface Tolerance Ablation Results:\n")
    f.write("Tolerance\tSurface %\tKL Divergence\n")
    for tolerance in tolerance_values:
        result = tolerance_results[tolerance]
        f.write(f"{tolerance}\t\t{result['surface_percentage']:.2f}\t\t{result['kl_divergence']:.4f}\n")

print("\nResults saved to 'results_summary.txt'")
