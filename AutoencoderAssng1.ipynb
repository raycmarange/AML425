{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJTYzc7BYe+wk7srLnuidW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raycmarange/AML425/blob/main/AutoencoderAssng1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrgGO_YteevY",
        "outputId": "e2590c81-f47d-4340-c3c9-76c207e1cb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1. Generating 3D Cube Surface Data ===\n",
            "\n",
            "=== 2. Building Autoencoder with MSE Objective and MMD Regularization ===\n",
            "\n",
            "=== Multi-seed Runs and Ablation Study ===\n",
            "\n",
            "--- Training with seed 42 ---\n",
            "Generated 6000 samples on cube surface\n",
            "Training autoencoder with latent dimension 1\n",
            "Epoch 0: loss=0.3952, val_loss=0.3323\n",
            "Epoch 20: loss=0.1279, val_loss=0.1227\n",
            "Epoch 40: loss=0.1013, val_loss=0.0979\n",
            "Epoch 60: loss=0.0913, val_loss=0.0902\n",
            "Epoch 80: loss=0.0869, val_loss=0.0845\n",
            "Training autoencoder with latent dimension 2\n",
            "Epoch 0: loss=0.2721, val_loss=0.1735\n",
            "Epoch 20: loss=0.0103, val_loss=0.0049\n",
            "Epoch 40: loss=0.0065, val_loss=0.0041\n",
            "Epoch 60: loss=0.0055, val_loss=0.0042\n",
            "Epoch 80: loss=0.0051, val_loss=0.0031\n",
            "Training autoencoder with latent dimension 4\n",
            "Epoch 0: loss=0.1278, val_loss=0.0014\n",
            "Epoch 20: loss=0.0070, val_loss=0.0002\n",
            "Epoch 40: loss=0.0064, val_loss=0.0002\n",
            "Epoch 60: loss=0.0065, val_loss=0.0002\n",
            "Epoch 80: loss=0.0065, val_loss=0.0001\n",
            "Training autoencoder with latent dimension 8\n",
            "Epoch 0: loss=0.1419, val_loss=0.0017\n",
            "Epoch 20: loss=0.0079, val_loss=0.0001\n",
            "Epoch 40: loss=0.0075, val_loss=0.0015\n",
            "Epoch 60: loss=0.0073, val_loss=0.0001\n",
            "Epoch 80: loss=0.0072, val_loss=0.0001\n",
            "Training autoencoder with latent dimension 16\n",
            "Epoch 0: loss=0.1288, val_loss=0.0011\n",
            "Epoch 20: loss=0.0070, val_loss=0.0002\n",
            "Epoch 40: loss=0.0067, val_loss=0.0003\n",
            "Epoch 60: loss=0.0067, val_loss=0.0001\n",
            "Epoch 80: loss=0.0065, val_loss=0.0001\n",
            "\n",
            "--- Training with seed 123 ---\n",
            "Generated 6000 samples on cube surface\n",
            "Training autoencoder with latent dimension 1\n",
            "Epoch 0: loss=0.3803, val_loss=0.3325\n",
            "Epoch 20: loss=0.1239, val_loss=0.1206\n",
            "Epoch 40: loss=0.0974, val_loss=0.0917\n",
            "Epoch 60: loss=0.0916, val_loss=0.0901\n",
            "Epoch 80: loss=0.0883, val_loss=0.0729\n",
            "Training autoencoder with latent dimension 2\n",
            "Epoch 0: loss=0.2696, val_loss=0.1527\n",
            "Epoch 20: loss=0.0088, val_loss=0.0051\n",
            "Epoch 40: loss=0.0065, val_loss=0.0033\n",
            "Epoch 60: loss=0.0053, val_loss=0.0017\n",
            "Epoch 80: loss=0.0045, val_loss=0.0013\n",
            "Training autoencoder with latent dimension 4\n",
            "Epoch 0: loss=0.1524, val_loss=0.0020\n",
            "Epoch 20: loss=0.0072, val_loss=0.0002\n",
            "Epoch 40: loss=0.0062, val_loss=0.0002\n",
            "Epoch 60: loss=0.0063, val_loss=0.0001\n",
            "Epoch 80: loss=0.0065, val_loss=0.0002\n",
            "Training autoencoder with latent dimension 8\n",
            "Epoch 0: loss=0.1430, val_loss=0.0013\n",
            "Epoch 20: loss=0.0078, val_loss=0.0003\n",
            "Epoch 40: loss=0.0075, val_loss=0.0001\n",
            "Epoch 60: loss=0.0072, val_loss=0.0001\n",
            "Epoch 80: loss=0.0073, val_loss=0.0001\n",
            "Training autoencoder with latent dimension 16\n",
            "Epoch 0: loss=0.1213, val_loss=0.0011\n",
            "Epoch 20: loss=0.0069, val_loss=0.0005\n",
            "Epoch 40: loss=0.0066, val_loss=0.0001\n",
            "Epoch 60: loss=0.0067, val_loss=0.0004\n",
            "Epoch 80: loss=0.0065, val_loss=0.0003\n",
            "\n",
            "--- Training with seed 456 ---\n",
            "Generated 6000 samples on cube surface\n",
            "Training autoencoder with latent dimension 1\n",
            "Epoch 0: loss=0.3910, val_loss=0.3316\n",
            "Epoch 20: loss=0.1444, val_loss=0.1474\n",
            "Epoch 40: loss=0.1154, val_loss=0.1100\n",
            "Epoch 60: loss=0.0985, val_loss=0.1002\n",
            "Epoch 80: loss=0.0932, val_loss=0.0907\n",
            "Training autoencoder with latent dimension 2\n",
            "Epoch 0: loss=0.2500, val_loss=0.1191\n",
            "Epoch 20: loss=0.0122, val_loss=0.0065\n",
            "Epoch 40: loss=0.0068, val_loss=0.0033\n",
            "Epoch 60: loss=0.0064, val_loss=0.0012\n",
            "Epoch 80: loss=0.0060, val_loss=0.0012\n",
            "Training autoencoder with latent dimension 4\n",
            "Epoch 0: loss=0.1520, val_loss=0.0023\n",
            "Epoch 20: loss=0.0072, val_loss=0.0003\n",
            "Epoch 40: loss=0.0063, val_loss=0.0002\n",
            "Epoch 60: loss=0.0062, val_loss=0.0002\n",
            "Epoch 80: loss=0.0064, val_loss=0.0002\n",
            "Training autoencoder with latent dimension 8\n",
            "Epoch 0: loss=0.1507, val_loss=0.0016\n",
            "Epoch 20: loss=0.0079, val_loss=0.0004\n",
            "Epoch 40: loss=0.0074, val_loss=0.0002\n",
            "Epoch 60: loss=0.0074, val_loss=0.0001\n",
            "Epoch 80: loss=0.0072, val_loss=0.0002\n",
            "Training autoencoder with latent dimension 16\n",
            "Epoch 0: loss=0.1246, val_loss=0.0009\n",
            "Epoch 20: loss=0.0071, val_loss=0.0003\n",
            "Epoch 40: loss=0.0068, val_loss=0.0002\n",
            "Epoch 60: loss=0.0067, val_loss=0.0001\n",
            "Epoch 80: loss=0.0065, val_loss=0.0001\n",
            "\n",
            "=== Multi-seed Results (Mean ± Std) ===\n",
            "Dim\tFinal Val Loss\t\tReconstruction Loss\tMMD Loss\n",
            "1\t0.079151 ± 0.001954\t0.079670 ± 0.000633\t0.041895 ± 0.004505\n",
            "2\t0.001729 ± 0.001157\t0.000896 ± 0.000335\t0.042148 ± 0.001933\n",
            "4\t0.000229 ± 0.000079\t0.000168 ± 0.000020\t0.060986 ± 0.001207\n",
            "8\t0.000142 ± 0.000046\t0.000130 ± 0.000041\t0.069940 ± 0.000258\n",
            "16\t0.000103 ± 0.000028\t0.000123 ± 0.000036\t0.063629 ± 0.000038\n",
            "\n",
            "=== Ablation Study: Effect of MMD Sigma ===\n",
            "\n",
            "--- Training with MMD sigma = 0.1 ---\n",
            "Epoch 0: loss=0.2660, val_loss=0.1615\n",
            "Epoch 20: loss=0.0157, val_loss=0.0080\n",
            "Epoch 40: loss=0.0080, val_loss=0.0013\n",
            "Epoch 60: loss=0.0073, val_loss=0.0015\n",
            "Epoch 80: loss=0.0069, val_loss=0.0006\n",
            "\n",
            "--- Training with MMD sigma = 0.5 ---\n",
            "Epoch 0: loss=0.2467, val_loss=0.1763\n",
            "Epoch 20: loss=0.0369, val_loss=0.0342\n",
            "Epoch 40: loss=0.0231, val_loss=0.0172\n",
            "Epoch 60: loss=0.0161, val_loss=0.0117\n",
            "Epoch 80: loss=0.0146, val_loss=0.0082\n",
            "\n",
            "--- Training with MMD sigma = 1.0 ---\n",
            "Epoch 0: loss=0.2613, val_loss=0.1516\n",
            "Epoch 20: loss=0.0102, val_loss=0.0033\n",
            "Epoch 40: loss=0.0070, val_loss=0.0031\n",
            "Epoch 60: loss=0.0063, val_loss=0.0025\n",
            "Epoch 80: loss=0.0059, val_loss=0.0024\n",
            "\n",
            "--- Training with MMD sigma = 2.0 ---\n",
            "Epoch 0: loss=0.2641, val_loss=0.1500\n",
            "Epoch 20: loss=0.0088, val_loss=0.0084\n",
            "Epoch 40: loss=0.0046, val_loss=0.0031\n",
            "Epoch 60: loss=0.0033, val_loss=0.0023\n",
            "Epoch 80: loss=0.0030, val_loss=0.0019\n",
            "\n",
            "--- Training with MMD sigma = 5.0 ---\n",
            "Epoch 0: loss=0.2662, val_loss=0.1443\n",
            "Epoch 20: loss=0.0092, val_loss=0.0098\n",
            "Epoch 40: loss=0.0049, val_loss=0.0041\n",
            "Epoch 60: loss=0.0033, val_loss=0.0019\n",
            "Epoch 80: loss=0.0019, val_loss=0.0010\n",
            "\n",
            "MMD Sigma Ablation Results:\n",
            "Sigma\tVal Loss\tSurface %\tKL Divergence\n",
            "0.1\t0.000427\t98.40\t\t0.1715\n",
            "0.5\t0.011036\t82.90\t\t0.1429\n",
            "1.0\t0.001268\t99.30\t\t0.2516\n",
            "2.0\t0.002589\t98.70\t\t0.2595\n",
            "5.0\t0.001050\t99.10\t\t0.2044\n",
            "\n",
            "=== Ablation Study: Effect of Surface Tolerance ===\n",
            "Using best sigma value: 0.5\n",
            "Epoch 0: loss=0.2638\n",
            "Epoch 20: loss=0.0268\n",
            "Epoch 40: loss=0.0141\n",
            "Epoch 60: loss=0.0105\n",
            "Epoch 80: loss=0.0093\n",
            "\n",
            "--- Evaluating with surface tolerance = 0.01 ---\n",
            "\n",
            "--- Evaluating with surface tolerance = 0.05 ---\n",
            "\n",
            "--- Evaluating with surface tolerance = 0.1 ---\n",
            "\n",
            "--- Evaluating with surface tolerance = 0.2 ---\n",
            "\n",
            "--- Evaluating with surface tolerance = 0.3 ---\n",
            "\n",
            "Surface Tolerance Ablation Results:\n",
            "Tolerance\tSurface %\tKL Divergence\n",
            "0.01\t\t25.90\t\t0.3214\n",
            "0.05\t\t91.30\t\t0.3011\n",
            "0.1\t\t98.60\t\t0.2475\n",
            "0.2\t\t99.90\t\t0.2584\n",
            "0.3\t\t99.60\t\t0.3891\n",
            "\n",
            "=== 3. Probabilistic Interpretation of MSE ===\n",
            "The MSE objective function can be interpreted as maximizing the log-likelihood\n",
            "of the data under a Gaussian distribution with fixed variance.\n",
            "Specifically, minimizing MSE is equivalent to maximizing:\n",
            "log p(X|Z) = -1/(2σ²) * MSE(X, X̂) + constant\n",
            "where σ² is the fixed variance of the Gaussian noise model.\n",
            "This assumes that the reconstruction errors are normally distributed with mean 0.\n",
            "This corresponds to Maximum Likelihood Estimation (MLE) under the Gaussian assumption.\n",
            "\n",
            "=== 4. Controlling Latent Distribution with Noise ===\n",
            "\n",
            "Information Rate at Different SNRs:\n",
            "SNR (dB)\tInfo Rate (bits/sample)\tSignal Power\tNoise Power\n",
            "-10\t\t0.0688\t\t\t0.9256\t\t9.2562\n",
            "-5\t\t0.1982\t\t\t0.9256\t\t2.9271\n",
            "0\t\t0.5000\t\t\t0.9256\t\t0.9256\n",
            "5\t\t1.0287\t\t\t0.9256\t\t0.2927\n",
            "10\t\t1.7297\t\t\t0.9256\t\t0.0926\n",
            "20\t\t3.3291\t\t\t0.9256\t\t0.0093\n",
            "30\t\t4.9836\t\t\t0.9256\t\t0.0009\n",
            "\n",
            "=== 5. Analyzing Reconstructions at Various SNRs and Latent Dimensions ===\n",
            "\n",
            "Reconstruction Quality Analysis:\n",
            "Dim\tSNR (dB)\tMSE\t\tInfo Rate (bits/sample)\n",
            "1\t-10\t\t1.397170\t0.0688\n",
            "1\t-5\t\t0.940749\t0.1982\n",
            "1\t0\t\t0.799372\t0.5000\n",
            "1\t5\t\t0.725933\t1.0287\n",
            "1\t10\t\t0.528414\t1.7297\n",
            "1\t20\t\t0.210515\t3.3291\n",
            "1\t30\t\t0.096566\t4.9836\n",
            "2\t-10\t\t1.529971\t0.0688\n",
            "2\t-5\t\t1.037850\t0.1982\n",
            "2\t0\t\t0.658811\t0.5000\n",
            "2\t5\t\t0.314675\t1.0287\n",
            "2\t10\t\t0.128608\t1.7297\n",
            "2\t20\t\t0.019259\t3.3291\n",
            "2\t30\t\t0.004805\t4.9836\n",
            "4\t-10\t\t4.384123\t0.0688\n",
            "4\t-5\t\t1.500645\t0.1982\n",
            "4\t0\t\t0.509571\t0.5000\n",
            "4\t5\t\t0.162904\t1.0287\n",
            "4\t10\t\t0.053631\t1.7297\n",
            "4\t20\t\t0.005130\t3.3291\n",
            "4\t30\t\t0.000615\t4.9836\n",
            "8\t-10\t\t78.645271\t0.0688\n",
            "8\t-5\t\t21.890739\t0.1982\n",
            "8\t0\t\t4.939592\t0.5000\n",
            "8\t5\t\t1.262145\t1.0287\n",
            "8\t10\t\t0.371742\t1.7297\n",
            "8\t20\t\t0.034517\t3.3291\n",
            "8\t30\t\t0.004416\t4.9836\n",
            "16\t-10\t\t7.889823\t0.0688\n",
            "16\t-5\t\t2.409122\t0.1982\n",
            "16\t0\t\t0.852716\t0.5000\n",
            "16\t5\t\t0.265639\t1.0287\n",
            "16\t10\t\t0.090418\t1.7297\n",
            "16\t20\t\t0.010098\t3.3291\n",
            "16\t30\t\t0.001180\t4.9836\n",
            "\n",
            "=== 6. Creating Generative System with Quality Measures ===\n",
            "\n",
            "Generative Performance Evaluation:\n",
            "Dim\tSurface %\tKL Divergence\n",
            "1\t17.00%\t\t6.9814\n",
            "2\t98.50%\t\t0.1829\n",
            "4\t19.30%\t\t0.2348\n",
            "8\t11.50%\t\t0.8171\n",
            "16\t20.60%\t\t0.1893\n",
            "\n",
            "=== 7. Improved Mutual Information Estimation ===\n",
            "Latent Dimension: 1, Mutual Information: 2.2579 bits\n",
            "Latent Dimension: 2, Mutual Information: 4.6956 bits\n",
            "Latent Dimension: 4, Mutual Information: 7.7216 bits\n",
            "Latent Dimension: 8, Mutual Information: 16.2168 bits\n",
            "Latent Dimension: 16, Mutual Information: 32.6052 bits\n",
            "\n",
            "=== 8. Comparison with VAE Approach ===\n",
            "Our noise-based approach controls information flow by adding Gaussian noise\n",
            "to the latent representation, which is similar to the VAE's approach of\n",
            "modeling the latent distribution as Gaussian.\n",
            "\n",
            "Key differences:\n",
            "1. Our approach explicitly controls SNR, while VAE uses KL divergence\n",
            "2. VAE learns both mean and variance of the latent distribution\n",
            "3. Our MMD regularization encourages Gaussian latent distribution like VAE\n",
            "4. VAE's ELBO objective combines reconstruction and distribution matching\n",
            "\n",
            "Both approaches aim to create a well-structured latent space suitable\n",
            "for generation, but VAE provides a more principled probabilistic framework.\n",
            "\n",
            "=== 10. Generating Missing Visualization Plots ===\n",
            "Generating 3D comparison plots...\n",
            "Generating validation loss curves...\n",
            "Generating information rate vs quality plot...\n",
            "Generating latent space visualizations...\n",
            "Generating training history plots...\n",
            "All missing visualizations have been generated and saved as PNG files.\n",
            "\n",
            "==================================================\n",
            "SUMMARY OF RESULTS\n",
            "==================================================\n",
            "\n",
            "1. Autoencoder Performance by Latent Dimension (Mean ± Std across seeds):\n",
            "Dim\tFinal MSE\tMutual Info (bits)\n",
            "1\t0.079151 ± 0.001954\t2.2579\n",
            "2\t0.001729 ± 0.001157\t4.6956\n",
            "4\t0.000229 ± 0.000079\t7.7216\n",
            "8\t0.000142 ± 0.000046\t16.2168\n",
            "16\t0.000103 ± 0.000028\t32.6052\n",
            "\n",
            "2. Information Rate at Different SNRs (for dim=2):\n",
            "SNR: -10 dB -> Info Rate: 0.0688 bits/sample\n",
            "SNR: -5 dB -> Info Rate: 0.1982 bits/sample\n",
            "SNR: 0 dB -> Info Rate: 0.5000 bits/sample\n",
            "SNR: 5 dB -> Info Rate: 1.0287 bits/sample\n",
            "SNR: 10 dB -> Info Rate: 1.7297 bits/sample\n",
            "SNR: 20 dB -> Info Rate: 3.3291 bits/sample\n",
            "SNR: 30 dB -> Info Rate: 4.9836 bits/sample\n",
            "\n",
            "3. Generative Performance:\n",
            "Dim\tSurface %\tKL Divergence\n",
            "1\t17.00%\t\t6.9814\n",
            "2\t98.50%\t\t0.1829\n",
            "4\t19.30%\t\t0.2348\n",
            "8\t11.50%\t\t0.8171\n",
            "16\t20.60%\t\t0.1893\n",
            "\n",
            "4. MMD Sigma Ablation Results:\n",
            "Sigma\tVal Loss\tSurface %\tKL Divergence\n",
            "0.1\t0.000427\t98.40\t\t0.1715\n",
            "0.5\t0.011036\t82.90\t\t0.1429\n",
            "1.0\t0.001268\t99.30\t\t0.2516\n",
            "2.0\t0.002589\t98.70\t\t0.2595\n",
            "5.0\t0.001050\t99.10\t\t0.2044\n",
            "\n",
            "5. Surface Tolerance Ablation Results:\n",
            "Tolerance\tSurface %\tKL Divergence\n",
            "0.01\t\t25.90\t\t0.3214\n",
            "0.05\t\t91.30\t\t0.3011\n",
            "0.1\t\t98.60\t\t0.2475\n",
            "0.2\t\t99.90\t\t0.2584\n",
            "0.3\t\t99.60\t\t0.3891\n",
            "\n",
            "6. Key Observations:\n",
            "- Higher latent dimensions allow more information preservation but may overfit\n",
            "- Very low SNRs (<0 dB) destroy most information and yield poor reconstructions\n",
            "- The optimal latent dimension for generation appears to be around 2-4\n",
            "- The autoencoder learns to capture the cube surface structure reasonably well\n",
            "- Information rate increases with SNR as expected from theory\n",
            "- MMD regularization helps structure the latent space for better generation\n",
            "- MMD sigma affects both reconstruction quality and generative performance\n",
            "- Surface tolerance parameter affects the evaluation of generative quality\n",
            "\n",
            "Results saved to 'results_summary.txt'\n"
          ]
        }
      ],
      "source": [
        "# Autoencoder Assignment 1\n",
        "# Name: [ray marange]\n",
        "# Date: [04/11/2024]\n",
        "# Email: [rayc.marange@gmail.com]\n",
        "# Description: Implementation of an autoencoder with MSE objective, latent space control,\n",
        "#              and generative capabilities, along with analysis of information-theoretic properties.\n",
        "# Note: This code is structured to follow the assignment requirements step-by-step.\n",
        "# University of VICTORIA WELLINGTON NEW ZEALAND\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "layers = tf.keras.layers\n",
        "models = tf.keras.models\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.stats import entropy, multivariate_normal\n",
        "import seaborn as sns\n",
        "from scipy.special import psi\n",
        "\n",
        "###############################################################################\n",
        "# 1. Create 3D data uniformly distributed over the surface of a cube\n",
        "###############################################################################\n",
        "print(\"=== 1. Generating 3D Cube Surface Data ===\")\n",
        "\n",
        "def generate_cube_surface_data(n_samples=10000):\n",
        "    \"\"\"\n",
        "    Generate points uniformly distributed on the surface of a cube centered at origin with side length 2\n",
        "    Returns:\n",
        "        numpy array of shape (n_samples, 3) containing the 3D points\n",
        "    \"\"\"\n",
        "    samples_per_face = n_samples // 6\n",
        "\n",
        "    data = []\n",
        "    # For each face of the cube (x=±1, y=±1, z=±1)\n",
        "    for fixed_coord in [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0, -1), (0, 0, 1)]:\n",
        "        if len(fixed_coord) == 2:  # x or y is fixed\n",
        "            axis, value = fixed_coord\n",
        "            if axis == -1:  # x = -1\n",
        "                x = np.full(samples_per_face, -1)\n",
        "                y = np.random.uniform(-1, 1, samples_per_face)\n",
        "                z = np.random.uniform(-1, 1, samples_per_face)\n",
        "            elif axis == 1:  # x = 1\n",
        "                x = np.full(samples_per_face, 1)\n",
        "                y = np.random.uniform(-1, 1, samples_per_face)\n",
        "                z = np.random.uniform(-1, 1, samples_per_face)\n",
        "            elif axis == 0 and value == -1:  # y = -1\n",
        "                x = np.random.uniform(-1, 1, samples_per_face)\n",
        "                y = np.full(samples_per_face, -1)\n",
        "                z = np.random.uniform(-1, 1, samples_per_face)\n",
        "            else:  # y = 1\n",
        "                x = np.random.uniform(-1, 1, samples_per_face)\n",
        "                y = np.full(samples_per_face, 1)\n",
        "                z = np.random.uniform(-1, 1, samples_per_face)\n",
        "        else:  # z is fixed\n",
        "            _, _, value = fixed_coord\n",
        "            if value == -1:  # z = -1\n",
        "                x = np.random.uniform(-1, 1, samples_per_face)\n",
        "                y = np.random.uniform(-1, 1, samples_per_face)\n",
        "                z = np.full(samples_per_face, -1)\n",
        "            else:  # z = 1\n",
        "                x = np.random.uniform(-1, 1, samples_per_face)\n",
        "                y = np.random.uniform(-1, 1, samples_per_face)\n",
        "                z = np.full(samples_per_face, 1)\n",
        "\n",
        "        face_data = np.column_stack((x, y, z))\n",
        "        data.append(face_data)\n",
        "\n",
        "    data = np.vstack(data)\n",
        "    np.random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "###############################################################################\n",
        "# 2. Autoencoder with MSE objective and adjustable latent size\n",
        "#    Enhanced with MMD regularization for latent space\n",
        "###############################################################################\n",
        "print(\"\\n=== 2. Building Autoencoder with MSE Objective and MMD Regularization ===\")\n",
        "\n",
        "def compute_mmd(x, y, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Compute Maximum Mean Discrepancy (MMD) between two samples\n",
        "    Uses Gaussian kernel with specified sigma\n",
        "    \"\"\"\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)  # Ensure float32\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)  # Ensure float32\n",
        "\n",
        "    xx = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.square(x[:, None] - x[None, :]), axis=-1) / (2 * sigma**2)))\n",
        "    yy = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.square(y[:, None] - y[None, :]), axis=-1) / (2 * sigma**2)))\n",
        "    xy = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.square(x[:, None] - y[None, :]), axis=-1) / (2 * sigma**2)))\n",
        "\n",
        "    return xx + yy - 2 * xy\n",
        "\n",
        "def build_autoencoder(latent_dim=2, mmd_weight=0.1, mmd_sigma=1.0):\n",
        "    \"\"\"\n",
        "    Build an autoencoder with adjustable latent dimension and MMD regularization\n",
        "    Args:\n",
        "        latent_dim: Dimension of the latent space\n",
        "        mmd_weight: Weight for the MMD regularization term\n",
        "        mmd_sigma: Sigma parameter for MMD Gaussian kernel\n",
        "    Returns:\n",
        "        autoencoder, encoder, decoder models\n",
        "    \"\"\"\n",
        "    # Encoder\n",
        "    encoder_input = layers.Input(shape=(3,))\n",
        "    x = layers.Dense(64, activation='relu')(encoder_input)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    latent = layers.Dense(latent_dim, activation='linear', name='latent')(x)\n",
        "    encoder_model = models.Model(encoder_input, latent)\n",
        "\n",
        "    # Decoder\n",
        "    latent_input = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(32, activation='relu')(latent_input)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    decoder_output = layers.Dense(3, activation='linear')(x)\n",
        "    decoder_model = models.Model(latent_input, decoder_output)\n",
        "\n",
        "    # Autoencoder\n",
        "    autoencoder_input = layers.Input(shape=(3,))\n",
        "    encoded = encoder_model(autoencoder_input)\n",
        "    decoded = decoder_model(encoded)\n",
        "    autoencoder_model = models.Model(autoencoder_input, decoded)\n",
        "\n",
        "    # Custom training with MMD regularization\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_metric = tf.keras.metrics.Mean(name='loss')\n",
        "    recon_loss_metric = tf.keras.metrics.Mean(name='reconstruction_loss')\n",
        "    mmd_loss_metric = tf.keras.metrics.Mean(name='mmd_loss')\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            latent = encoder_model(data, training=True)\n",
        "            reconstructed = decoder_model(latent, training=True)\n",
        "\n",
        "            # Reconstruction loss\n",
        "            reconstruction_loss = tf.reduce_mean(tf.square(data - reconstructed))\n",
        "\n",
        "            # MMD regularization - match latent distribution to standard normal\n",
        "            true_samples = tf.random.normal(tf.shape(latent))\n",
        "            mmd_loss = compute_mmd(latent, true_samples, sigma=mmd_sigma)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = reconstruction_loss + mmd_weight * mmd_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        grads = tape.gradient(total_loss,\n",
        "                             encoder_model.trainable_variables + decoder_model.trainable_variables)\n",
        "        optimizer.apply_gradients(\n",
        "            zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        loss_metric.update_state(total_loss)\n",
        "        recon_loss_metric.update_state(reconstruction_loss)\n",
        "        mmd_loss_metric.update_state(mmd_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss_metric.result(),\n",
        "            \"reconstruction_loss\": recon_loss_metric.result(),\n",
        "            \"mmd_loss\": mmd_loss_metric.result(),\n",
        "        }\n",
        "\n",
        "    # Create a custom model class to handle training\n",
        "    class CustomAutoencoder(models.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super(CustomAutoencoder, self).__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "\n",
        "        def call(self, inputs):\n",
        "            latent = self.encoder(inputs)\n",
        "            return self.decoder(latent)\n",
        "\n",
        "        def train_step(self, data):\n",
        "            return train_step(data)\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [loss_metric, recon_loss_metric, mmd_loss_metric]\n",
        "\n",
        "    # Create and compile the model\n",
        "    autoencoder = CustomAutoencoder(encoder_model, decoder_model)\n",
        "    autoencoder.compile(optimizer=optimizer)\n",
        "\n",
        "    return autoencoder, encoder_model, decoder_model\n",
        "\n",
        "###############################################################################\n",
        "# Multi-seed runs and ablation study\n",
        "###############################################################################\n",
        "print(\"\\n=== Multi-seed Runs and Ablation Study ===\")\n",
        "\n",
        "# Define seeds for multi-seed runs\n",
        "seeds = [42, 123, 456]\n",
        "latent_dims = [1, 2, 4, 8, 16]\n",
        "mmd_weight = 0.1\n",
        "\n",
        "# Store results for each seed\n",
        "all_results = {}\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\n--- Training with seed {seed} ---\")\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Generate the data\n",
        "    cube_data = generate_cube_surface_data(6000)\n",
        "    cube_data = cube_data.astype(np.float32)\n",
        "    print(f\"Generated {cube_data.shape[0]} samples on cube surface\")\n",
        "\n",
        "    # Split into train and test\n",
        "    train_data, test_data = train_test_split(cube_data, test_size=0.2, random_state=seed)\n",
        "\n",
        "    autoencoders = {}\n",
        "    histories = {}\n",
        "\n",
        "    for dim in latent_dims:\n",
        "        print(f\"Training autoencoder with latent dimension {dim}\")\n",
        "        autoencoder, encoder, decoder = build_autoencoder(latent_dim=dim, mmd_weight=mmd_weight)\n",
        "\n",
        "        # Custom training loop\n",
        "        epochs = 100\n",
        "        batch_size = 32\n",
        "        n_batches = int(np.ceil(len(train_data) / batch_size))\n",
        "\n",
        "        history = {'loss': [], 'reconstruction_loss': [], 'mmd_loss': [], 'val_loss': []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle training data\n",
        "            indices = np.random.permutation(len(train_data))\n",
        "            train_data_shuffled = train_data[indices]\n",
        "\n",
        "            # Reset metrics\n",
        "            for metric in autoencoder.metrics:\n",
        "                metric.reset_state()\n",
        "\n",
        "            # Train on batches\n",
        "            for batch_idx in range(n_batches):\n",
        "                batch_start = batch_idx * batch_size\n",
        "                batch_end = min((batch_idx + 1) * batch_size, len(train_data))\n",
        "                batch_data = train_data_shuffled[batch_start:batch_end]\n",
        "\n",
        "                # Train step\n",
        "                autoencoder.train_step(batch_data)\n",
        "\n",
        "            # Record training metrics\n",
        "            history['loss'].append(autoencoder.metrics[0].result().numpy())\n",
        "            history['reconstruction_loss'].append(autoencoder.metrics[1].result().numpy())\n",
        "            history['mmd_loss'].append(autoencoder.metrics[2].result().numpy())\n",
        "\n",
        "            # Calculate validation loss\n",
        "            val_reconstructions = autoencoder.predict(test_data, verbose=0)\n",
        "            val_loss = np.mean(np.square(test_data - val_reconstructions))\n",
        "            history['val_loss'].append(val_loss)\n",
        "\n",
        "            if epoch % 20 == 0:\n",
        "                print(f\"Epoch {epoch}: loss={history['loss'][-1]:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "        autoencoders[dim] = (autoencoder, encoder, decoder)\n",
        "        histories[dim] = history\n",
        "\n",
        "    # Store results for this seed\n",
        "    all_results[seed] = {\n",
        "        'autoencoders': autoencoders,\n",
        "        'histories': histories,\n",
        "        'train_data': train_data,\n",
        "        'test_data': test_data\n",
        "    }\n",
        "\n",
        "# Calculate mean ± std across seeds for each latent dimension\n",
        "print(\"\\n=== Multi-seed Results (Mean ± Std) ===\")\n",
        "print(\"Dim\\tFinal Val Loss\\t\\tReconstruction Loss\\tMMD Loss\")\n",
        "for dim in latent_dims:\n",
        "    val_losses = []\n",
        "    recon_losses = []\n",
        "    mmd_losses = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        val_loss = all_results[seed]['histories'][dim]['val_loss'][-1]\n",
        "        recon_loss = all_results[seed]['histories'][dim]['reconstruction_loss'][-1]\n",
        "        mmd_loss = all_results[seed]['histories'][dim]['mmd_loss'][-1]\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        recon_losses.append(recon_loss)\n",
        "        mmd_losses.append(mmd_loss)\n",
        "\n",
        "    print(f\"{dim}\\t{np.mean(val_losses):.6f} ± {np.std(val_losses):.6f}\\t\"\n",
        "          f\"{np.mean(recon_losses):.6f} ± {np.std(recon_losses):.6f}\\t\"\n",
        "          f\"{np.mean(mmd_losses):.6f} ± {np.std(mmd_losses):.6f}\")\n",
        "\n",
        "###############################################################################\n",
        "# Ablation study: Effect of MMD sigma\n",
        "###############################################################################\n",
        "print(\"\\n=== Ablation Study: Effect of MMD Sigma ===\")\n",
        "\n",
        "# Use a fixed seed for ablation study\n",
        "ablation_seed = 42\n",
        "np.random.seed(ablation_seed)\n",
        "tf.random.set_seed(ablation_seed)\n",
        "\n",
        "# Generate data\n",
        "cube_data = generate_cube_surface_data(6000)\n",
        "cube_data = cube_data.astype(np.float32)\n",
        "train_data, test_data = train_test_split(cube_data, test_size=0.2, random_state=ablation_seed)\n",
        "\n",
        "# Test different MMD sigma values\n",
        "mmd_sigma_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "latent_dim = 2  # Fixed latent dimension for ablation study\n",
        "mmd_weight = 0.1\n",
        "\n",
        "sigma_results = {}\n",
        "\n",
        "for sigma in mmd_sigma_values:\n",
        "    print(f\"\\n--- Training with MMD sigma = {sigma} ---\")\n",
        "    autoencoder, encoder, decoder = build_autoencoder(latent_dim=latent_dim, mmd_weight=mmd_weight, mmd_sigma=sigma)\n",
        "\n",
        "    # Custom training loop\n",
        "    epochs = 100\n",
        "    batch_size = 32\n",
        "    n_batches = int(np.ceil(len(train_data) / batch_size))\n",
        "\n",
        "    history = {'loss': [], 'reconstruction_loss': [], 'mmd_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        indices = np.random.permutation(len(train_data))\n",
        "        train_data_shuffled = train_data[indices]\n",
        "\n",
        "        # Reset metrics\n",
        "        for metric in autoencoder.metrics:\n",
        "            metric.reset_state()\n",
        "\n",
        "        # Train on batches\n",
        "        for batch_idx in range(n_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min((batch_idx + 1) * batch_size, len(train_data))\n",
        "            batch_data = train_data_shuffled[batch_start:batch_end]\n",
        "\n",
        "            # Train step\n",
        "            autoencoder.train_step(batch_data)\n",
        "\n",
        "        # Record training metrics\n",
        "        history['loss'].append(autoencoder.metrics[0].result().numpy())\n",
        "        history['reconstruction_loss'].append(autoencoder.metrics[1].result().numpy())\n",
        "        history['mmd_loss'].append(autoencoder.metrics[2].result().numpy())\n",
        "\n",
        "        # Calculate validation loss\n",
        "        val_reconstructions = autoencoder.predict(test_data, verbose=0)\n",
        "        val_loss = np.mean(np.square(test_data - val_reconstructions))\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}: loss={history['loss'][-1]:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "    # Evaluate generative performance\n",
        "    def create_generative_system(encoder, decoder, data):\n",
        "        \"\"\"\n",
        "        Create a generative system from the autoencoder\n",
        "        Args:\n",
        "            encoder: Encoder model\n",
        "            decoder: Decoder model\n",
        "            data: Training data to fit the latent distribution\n",
        "        Returns:\n",
        "            generate_samples: Function to generate new samples\n",
        "            latent_mean: Mean of the latent distribution\n",
        "            latent_cov: Covariance of the latent distribution\n",
        "        \"\"\"\n",
        "        # Get latent representations of training data\n",
        "        latent_codes = encoder.predict(data, verbose=0)\n",
        "\n",
        "        # Fit a Gaussian distribution to the latent codes\n",
        "        latent_mean = np.mean(latent_codes, axis=0)\n",
        "        latent_cov = np.cov(latent_codes, rowvar=False)\n",
        "\n",
        "        # Ensure covariance matrix is positive definite\n",
        "        if len(latent_mean) > 1:\n",
        "            # Add a small value to the diagonal to ensure positive definiteness\n",
        "            latent_cov += np.eye(latent_cov.shape[0]) * 1e-6\n",
        "\n",
        "        def generate_samples(n_samples):\n",
        "            # Sample from the learned latent distribution\n",
        "            if len(latent_mean) == 1:\n",
        "                # 1D case\n",
        "                latent_samples = np.random.normal(latent_mean[0], np.sqrt(latent_cov), (n_samples, 1))\n",
        "            else:\n",
        "                # Multi-dimensional case\n",
        "                try:\n",
        "                    latent_samples = np.random.multivariate_normal(latent_mean, latent_cov, n_samples)\n",
        "                except:\n",
        "                    # If covariance is not positive definite, use diagonal approximation\n",
        "                    latent_cov_diag = np.diag(np.diag(latent_cov))\n",
        "                    latent_samples = np.random.multivariate_normal(latent_mean, latent_cov_diag, n_samples)\n",
        "\n",
        "            # Decode to generate new samples\n",
        "            generated_samples = decoder.predict(latent_samples, verbose=0)\n",
        "            return generated_samples\n",
        "\n",
        "        return generate_samples, latent_mean, latent_cov\n",
        "\n",
        "    def is_on_cube_surface(point, tolerance=0.1):\n",
        "        \"\"\"\n",
        "        Check if a point is on the cube surface within tolerance\n",
        "        Args:\n",
        "            point: 3D point to check\n",
        "            tolerance: Tolerance for being on the surface\n",
        "        Returns:\n",
        "            Boolean indicating if the point is on the cube surface\n",
        "        \"\"\"\n",
        "        # A point is on the cube surface if at least one coordinate is ±1 within tolerance\n",
        "        # and the other coordinates are within [-1, 1]\n",
        "        on_surface = False\n",
        "        for i in range(3):\n",
        "            if abs(abs(point[i]) - 1) < tolerance:\n",
        "                # Check if other coordinates are within bounds\n",
        "                other_coords = [j for j in range(3) if j != i]\n",
        "                if all(-1 <= point[j] <= 1 for j in other_coords):\n",
        "                    on_surface = True\n",
        "                    break\n",
        "        return on_surface\n",
        "\n",
        "    def kl_divergence(p, q):\n",
        "        \"\"\"\n",
        "        Compute KL divergence between two distributions\n",
        "        Args:\n",
        "            p: First distribution (reference)\n",
        "            q: Second distribution (approximation)\n",
        "        Returns:\n",
        "            KL divergence value\n",
        "        \"\"\"\n",
        "        # Ensure distributions are normalized\n",
        "        p = p / np.sum(p)\n",
        "        q = q / np.sum(q)\n",
        "\n",
        "        # Avoid zeros for KL calculation\n",
        "        p = np.clip(p, 1e-10, 1)\n",
        "        q = np.clip(q, 1e-10, 1)\n",
        "\n",
        "        return np.sum(p * np.log(p / q))\n",
        "\n",
        "    def evaluate_generative_performance(generate_samples, n_samples=1000, tolerance=0.1):\n",
        "        \"\"\"\n",
        "        Evaluate the quality of generated samples\n",
        "        Args:\n",
        "            generate_samples: Function to generate samples\n",
        "            n_samples: Number of samples to generate\n",
        "            tolerance: Tolerance for surface detection\n",
        "        Returns:\n",
        "            surface_percentage: Percentage of samples on cube surface\n",
        "            kl_divergence: KL divergence between original and generated distributions\n",
        "            samples: Generated samples\n",
        "        \"\"\"\n",
        "        # Generate samples\n",
        "        samples = generate_samples(n_samples)\n",
        "\n",
        "        # Calculate percentage of samples on cube surface\n",
        "        on_surface_count = 0\n",
        "        for sample in samples:\n",
        "            if is_on_cube_surface(sample, tolerance):\n",
        "                on_surface_count += 1\n",
        "\n",
        "        surface_percentage = on_surface_count / n_samples * 100\n",
        "\n",
        "        # Calculate distribution similarity using KL divergence\n",
        "        # We'll compare the distribution of distances from origin and angles\n",
        "        original_distances = np.linalg.norm(train_data, axis=1)\n",
        "        generated_distances = np.linalg.norm(samples, axis=1)\n",
        "\n",
        "        # Calculate histogram-based KL divergence for distances\n",
        "        hist_original_dist, bin_edges_dist = np.histogram(original_distances, bins=50, density=True)\n",
        "        hist_generated_dist, _ = np.histogram(generated_distances, bins=bin_edges_dist, density=True)\n",
        "\n",
        "        kl_dist = kl_divergence(hist_original_dist, hist_generated_dist)\n",
        "\n",
        "        # Calculate KL divergence for angular distribution (if 2D or 3D)\n",
        "        if samples.shape[1] >= 2:\n",
        "            # For 3D data, we can compute angles\n",
        "            original_angles = np.arctan2(train_data[:, 1], train_data[:, 0])\n",
        "            generated_angles = np.arctan2(samples[:, 1], samples[:, 0])\n",
        "\n",
        "            hist_original_ang, bin_edges_ang = np.histogram(original_angles, bins=50, density=True)\n",
        "            hist_generated_ang, _ = np.histogram(generated_angles, bins=bin_edges_ang, density=True)\n",
        "\n",
        "            kl_ang = kl_divergence(hist_original_ang, hist_generated_ang)\n",
        "\n",
        "            # Combine KL divergences\n",
        "            total_kl = kl_dist + kl_ang\n",
        "        else:\n",
        "            total_kl = kl_dist\n",
        "\n",
        "        return surface_percentage, total_kl, samples\n",
        "\n",
        "    generate_samples, _, _ = create_generative_system(encoder, decoder, train_data)\n",
        "    surface_percentage, kl_value, _ = evaluate_generative_performance(generate_samples)\n",
        "\n",
        "    sigma_results[sigma] = {\n",
        "        'final_val_loss': history['val_loss'][-1],\n",
        "        'surface_percentage': surface_percentage,\n",
        "        'kl_divergence': kl_value\n",
        "    }\n",
        "\n",
        "# Plot results for MMD sigma ablation\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Final validation loss\n",
        "plt.subplot(1, 3, 1)\n",
        "sigma_values = list(sigma_results.keys())\n",
        "val_losses = [sigma_results[s]['final_val_loss'] for s in sigma_values]\n",
        "plt.plot(sigma_values, val_losses, 'o-')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('MMD Sigma')\n",
        "plt.ylabel('Final Validation Loss')\n",
        "plt.title('Effect of MMD Sigma on Validation Loss')\n",
        "\n",
        "# Surface percentage\n",
        "plt.subplot(1, 3, 2)\n",
        "surface_percentages = [sigma_results[s]['surface_percentage'] for s in sigma_values]\n",
        "plt.plot(sigma_values, surface_percentages, 'o-')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('MMD Sigma')\n",
        "plt.ylabel('Surface Percentage (%)')\n",
        "plt.title('Effect of MMD Sigma on Surface Percentage')\n",
        "\n",
        "# KL divergence\n",
        "plt.subplot(1, 3, 3)\n",
        "kl_values = [sigma_results[s]['kl_divergence'] for s in sigma_values]\n",
        "plt.plot(sigma_values, kl_values, 'o-')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('MMD Sigma')\n",
        "plt.ylabel('KL Divergence')\n",
        "plt.title('Effect of MMD Sigma on KL Divergence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ablation_mmd_sigma.png')\n",
        "plt.close()\n",
        "\n",
        "# Print ablation results\n",
        "print(\"\\nMMD Sigma Ablation Results:\")\n",
        "print(\"Sigma\\tVal Loss\\tSurface %\\tKL Divergence\")\n",
        "for sigma in mmd_sigma_values:\n",
        "    result = sigma_results[sigma]\n",
        "    print(f\"{sigma}\\t{result['final_val_loss']:.6f}\\t{result['surface_percentage']:.2f}\\t\\t{result['kl_divergence']:.4f}\")\n",
        "\n",
        "###############################################################################\n",
        "# Ablation study: Effect of surface tolerance\n",
        "###############################################################################\n",
        "print(\"\\n=== Ablation Study: Effect of Surface Tolerance ===\")\n",
        "\n",
        "# Use the best model from the sigma ablation study\n",
        "best_sigma = min(sigma_results, key=lambda k: sigma_results[k]['kl_divergence'])\n",
        "print(f\"Using best sigma value: {best_sigma}\")\n",
        "\n",
        "# Train a model with the best sigma\n",
        "autoencoder, encoder, decoder = build_autoencoder(latent_dim=latent_dim, mmd_weight=mmd_weight, mmd_sigma=best_sigma)\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "n_batches = int(np.ceil(len(train_data) / batch_size))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data\n",
        "    indices = np.random.permutation(len(train_data))\n",
        "    train_data_shuffled = train_data[indices]\n",
        "\n",
        "    # Reset metrics\n",
        "    for metric in autoencoder.metrics:\n",
        "        metric.reset_state()\n",
        "\n",
        "    # Train on batches\n",
        "    for batch_idx in range(n_batches):\n",
        "        batch_start = batch_idx * batch_size\n",
        "        batch_end = min((batch_idx + 1) * batch_size, len(train_data))\n",
        "        batch_data = train_data_shuffled[batch_start:batch_end]\n",
        "\n",
        "        # Train step\n",
        "        autoencoder.train_step(batch_data)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}: loss={autoencoder.metrics[0].result().numpy():.4f}\")\n",
        "\n",
        "# Test different surface tolerance values\n",
        "tolerance_values = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "tolerance_results = {}\n",
        "\n",
        "generate_samples, _, _ = create_generative_system(encoder, decoder, train_data)\n",
        "\n",
        "for tolerance in tolerance_values:\n",
        "    print(f\"\\n--- Evaluating with surface tolerance = {tolerance} ---\")\n",
        "    surface_percentage, kl_value, samples = evaluate_generative_performance(generate_samples, tolerance=tolerance)\n",
        "\n",
        "    tolerance_results[tolerance] = {\n",
        "        'surface_percentage': surface_percentage,\n",
        "        'kl_divergence': kl_value\n",
        "    }\n",
        "\n",
        "# Plot results for surface tolerance ablation\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Surface percentage\n",
        "plt.subplot(1, 2, 1)\n",
        "tolerance_values_list = list(tolerance_results.keys())\n",
        "surface_percentages = [tolerance_results[t]['surface_percentage'] for t in tolerance_values_list]\n",
        "plt.plot(tolerance_values_list, surface_percentages, 'o-')\n",
        "plt.xlabel('Surface Tolerance')\n",
        "plt.ylabel('Surface Percentage (%)')\n",
        "plt.title('Effect of Surface Tolerance on Surface Percentage')\n",
        "\n",
        "# KL divergence\n",
        "plt.subplot(1, 2, 2)\n",
        "kl_values = [tolerance_results[t]['kl_divergence'] for t in tolerance_values_list]\n",
        "plt.plot(tolerance_values_list, kl_values, 'o-')\n",
        "plt.xlabel('Surface Tolerance')\n",
        "plt.ylabel('KL Divergence')\n",
        "plt.title('Effect of Surface Tolerance on KL Divergence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ablation_surface_tolerance.png')\n",
        "plt.close()\n",
        "\n",
        "# Print tolerance ablation results\n",
        "print(\"\\nSurface Tolerance Ablation Results:\")\n",
        "print(\"Tolerance\\tSurface %\\tKL Divergence\")\n",
        "for tolerance in tolerance_values:\n",
        "    result = tolerance_results[tolerance]\n",
        "    print(f\"{tolerance}\\t\\t{result['surface_percentage']:.2f}\\t\\t{result['kl_divergence']:.4f}\")\n",
        "\n",
        "###############################################################################\n",
        "# 3. Probabilistic interpretation of MSE objective function\n",
        "###############################################################################\n",
        "print(\"\\n=== 3. Probabilistic Interpretation of MSE ===\")\n",
        "print(\"The MSE objective function can be interpreted as maximizing the log-likelihood\")\n",
        "print(\"of the data under a Gaussian distribution with fixed variance.\")\n",
        "print(\"Specifically, minimizing MSE is equivalent to maximizing:\")\n",
        "print(\"log p(X|Z) = -1/(2σ²) * MSE(X, X̂) + constant\")\n",
        "print(\"where σ² is the fixed variance of the Gaussian noise model.\")\n",
        "print(\"This assumes that the reconstruction errors are normally distributed with mean 0.\")\n",
        "print(\"This corresponds to Maximum Likelihood Estimation (MLE) under the Gaussian assumption.\")\n",
        "\n",
        "###############################################################################\n",
        "# 4. Method to control latent distribution and add noise\n",
        "###############################################################################\n",
        "print(\"\\n=== 4. Controlling Latent Distribution with Noise ===\")\n",
        "\n",
        "def add_latent_noise(encoder, data, snr_db):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise to latent representation with specified SNR\n",
        "    Returns noisy latent codes and the information rate\n",
        "    Args:\n",
        "        encoder: Encoder model\n",
        "        data: Input data\n",
        "        snr_db: Signal-to-noise ratio in dB\n",
        "    Returns:\n",
        "        noisy_latent: Latent codes with added noise\n",
        "        total_information: Information rate in bits/sample\n",
        "        signal_power: Power of the signal\n",
        "        noise_power: Power of the noise\n",
        "    \"\"\"\n",
        "    # Get latent representation\n",
        "    latent_codes = encoder.predict(data, verbose=0)\n",
        "\n",
        "    # Calculate signal power (variance)\n",
        "    signal_power = np.var(latent_codes, axis=0)\n",
        "\n",
        "    # Convert SNR from dB to linear scale\n",
        "    snr_linear = 10**(snr_db / 10)\n",
        "\n",
        "    # Calculate noise power for each dimension\n",
        "    noise_power = signal_power / snr_linear\n",
        "\n",
        "    # Add Gaussian noise\n",
        "    noise = np.random.normal(0, np.sqrt(noise_power), latent_codes.shape)\n",
        "    noisy_latent = latent_codes + noise\n",
        "\n",
        "    # Calculate information rate using equation (2)\n",
        "    # I(Y;Z) = 0.5 * log2(σ_z² / σ_ε²) = 0.5 * log2(SNR) for each dimension\n",
        "    information_rate = 0.5 * np.log2(1 + snr_linear)  # bits per dimension per sample\n",
        "\n",
        "    total_information = np.sum(information_rate)\n",
        "\n",
        "    return noisy_latent, total_information, signal_power, noise_power\n",
        "\n",
        "# Test with different SNRs using the best model from the first seed\n",
        "encoder = all_results[seeds[0]]['autoencoders'][2][1]  # Use dim=2 from first seed\n",
        "test_data = all_results[seeds[0]]['test_data']\n",
        "\n",
        "snr_values = [-10, -5, 0, 5, 10, 20, 30]  # in dB\n",
        "\n",
        "print(\"\\nInformation Rate at Different SNRs:\")\n",
        "print(\"SNR (dB)\\tInfo Rate (bits/sample)\\tSignal Power\\tNoise Power\")\n",
        "for snr_db in snr_values:\n",
        "    _, info_rate, signal_power, noise_power = add_latent_noise(encoder, test_data, snr_db)\n",
        "    print(f\"{snr_db}\\t\\t{info_rate:.4f}\\t\\t\\t{np.mean(signal_power):.4f}\\t\\t{np.mean(noise_power):.4f}\")\n",
        "\n",
        "###############################################################################\n",
        "# 5. Analyze reconstructions at various SNRs and latent dimensions\n",
        "###############################################################################\n",
        "print(\"\\n=== 5. Analyzing Reconstructions at Various SNRs and Latent Dimensions ===\")\n",
        "\n",
        "def evaluate_reconstruction(encoder, decoder, data, snr_db):\n",
        "    \"\"\"\n",
        "    Evaluate reconstruction quality with added noise\n",
        "    Args:\n",
        "        encoder: Encoder model\n",
        "        decoder: Decoder model\n",
        "        data: Input data\n",
        "        snr_db: Signal-to-noise ratio in dB\n",
        "    Returns:\n",
        "        mse: Mean squared error of reconstruction\n",
        "        info_rate: Information rate in bits/sample\n",
        "        reconstructed: Reconstructed data\n",
        "    \"\"\"\n",
        "    # Get noisy latent codes\n",
        "    noisy_latent, info_rate, _, _ = add_latent_noise(encoder, data, snr_db)\n",
        "\n",
        "    # Reconstruct\n",
        "    reconstructed = decoder.predict(noisy_latent, verbose=0)\n",
        "\n",
        "    # Calculate reconstruction error\n",
        "    mse = np.mean(np.square(data - reconstructed))\n",
        "\n",
        "    return mse, info_rate, reconstructed\n",
        "\n",
        "# Evaluate for different latent dimensions and SNRs using the first seed\n",
        "results = {}\n",
        "autoencoders_seed0 = all_results[seeds[0]]['autoencoders']\n",
        "test_data_seed0 = all_results[seeds[0]]['test_data']\n",
        "\n",
        "print(\"\\nReconstruction Quality Analysis:\")\n",
        "print(\"Dim\\tSNR (dB)\\tMSE\\t\\tInfo Rate (bits/sample)\")\n",
        "for dim in latent_dims:\n",
        "    encoder = autoencoders_seed0[dim][1]\n",
        "    decoder = autoencoders_seed0[dim][2]\n",
        "\n",
        "    results[dim] = {}\n",
        "\n",
        "    for snr_db in snr_values:\n",
        "        mse, info_rate, reconstructed = evaluate_reconstruction(encoder, decoder, test_data_seed0, snr_db)\n",
        "        results[dim][snr_db] = {'mse': mse, 'info_rate': info_rate}\n",
        "        print(f\"{dim}\\t{snr_db}\\t\\t{mse:.6f}\\t{info_rate:.4f}\")\n",
        "\n",
        "# Visualize reconstructions for a specific latent dimension and SNR\n",
        "dim = 2\n",
        "snr_db = 10\n",
        "encoder = autoencoders_seed0[dim][1]\n",
        "decoder = autoencoders_seed0[dim][2]\n",
        "\n",
        "_, _, reconstructed = evaluate_reconstruction(encoder, decoder, test_data_seed0, snr_db)\n",
        "\n",
        "# Plot original and reconstructed\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original data\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "sample_idx = np.random.choice(len(test_data_seed0), 500, replace=False)\n",
        "ax1.scatter(test_data_seed0[sample_idx, 0], test_data_seed0[sample_idx, 1], test_data_seed0[sample_idx, 2], alpha=0.5)\n",
        "ax1.set_title('Original Test Data')\n",
        "ax1.set_xlim(-1.5, 1.5)\n",
        "ax1.set_ylim(-1.5, 1.5)\n",
        "ax1.set_zlim(-1.5, 1.5)\n",
        "\n",
        "# Reconstructed data\n",
        "ax2 = fig.add_subplot(132, projection='3d')\n",
        "ax2.scatter(reconstructed[sample_idx, 0], reconstructed[sample_idx, 1], reconstructed[sample_idx, 2], alpha=0.5)\n",
        "ax2.set_title(f'Reconstructed (Dim={dim}, SNR={snr_db}dB)')\n",
        "ax2.set_xlim(-1.5, 1.5)\n",
        "ax2.set_ylim(-1.5, 1.5)\n",
        "ax2.set_zlim(-1.5, 1.5)\n",
        "\n",
        "# Error visualization\n",
        "errors = np.linalg.norm(test_data_seed0 - reconstructed, axis=1)\n",
        "ax3 = fig.add_subplot(133)\n",
        "ax3.hist(errors, bins=50)\n",
        "ax3.set_title('Reconstruction Error Distribution')\n",
        "ax3.set_xlabel('Euclidean Distance Error')\n",
        "ax3.set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('reconstruction_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "###############################################################################\n",
        "# 6. Create a generative system and define quality measures\n",
        "###############################################################################\n",
        "print(\"\\n=== 6. Creating Generative System with Quality Measures ===\")\n",
        "\n",
        "# Evaluate generative performance for different latent dimensions using the first seed\n",
        "print(\"\\nGenerative Performance Evaluation:\")\n",
        "print(\"Dim\\tSurface %\\tKL Divergence\")\n",
        "generative_performance = {}\n",
        "\n",
        "for dim in latent_dims:\n",
        "    encoder = autoencoders_seed0[dim][1]\n",
        "    decoder = autoencoders_seed0[dim][2]\n",
        "    train_data_seed0 = all_results[seeds[0]]['train_data']\n",
        "\n",
        "    generate_samples, latent_mean, latent_cov = create_generative_system(encoder, decoder, train_data_seed0)\n",
        "    surface_percentage, kl_value, samples = evaluate_generative_performance(generate_samples)\n",
        "\n",
        "    generative_performance[dim] = {\n",
        "        'surface_percentage': surface_percentage,\n",
        "        'kl_divergence': kl_value\n",
        "    }\n",
        "    print(f\"{dim}\\t{surface_percentage:.2f}%\\t\\t{kl_value:.4f}\")\n",
        "\n",
        "# Visualize generated samples for the best model\n",
        "best_dim = min(generative_performance, key=lambda k: generative_performance[k]['kl_divergence'])\n",
        "encoder = autoencoders_seed0[best_dim][1]\n",
        "decoder = autoencoders_seed0[best_dim][2]\n",
        "generate_samples, _, _ = create_generative_system(encoder, decoder, train_data_seed0)\n",
        "_, _, generated_samples = evaluate_generative_performance(generate_samples, 1000)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(generated_samples[:, 0], generated_samples[:, 1], generated_samples[:, 2], alpha=0.5)\n",
        "ax.set_title(f'Generated Samples (Best Model: Dim={best_dim})')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_xlim(-1.5, 1.5)\n",
        "ax.set_ylim(-1.5, 1.5)\n",
        "ax.set_zlim(-1.5, 1.5)\n",
        "plt.savefig('generated_samples.png')\n",
        "plt.close()\n",
        "\n",
        "###############################################################################\n",
        "# 7. Improved Mutual Information Estimation using k-NN method\n",
        "###############################################################################\n",
        "print(\"\\n=== 7. Improved Mutual Information Estimation ===\")\n",
        "\n",
        "def knn_mi_estimation(x, y, k=5):\n",
        "    \"\"\"\n",
        "    Estimate mutual information between x and y using k-NN method\n",
        "    Based on Kraskov et al. (2004) method\n",
        "    Args:\n",
        "        x: First variable (n_samples, n_features_x)\n",
        "        y: Second variable (n_samples, n_features_y)\n",
        "        k: Number of nearest neighbors to use\n",
        "    Returns:\n",
        "        mi: Estimated mutual information in bits\n",
        "    \"\"\"\n",
        "    n_samples = x.shape[0]\n",
        "\n",
        "    # Combine x and y\n",
        "    xy = np.hstack((x, y))\n",
        "\n",
        "    # Find k-nearest neighbors in the joint space\n",
        "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(xy)\n",
        "    distances, _ = nbrs.kneighbors(xy)\n",
        "\n",
        "    # The k+1-th neighbor is the farthest (we exclude the point itself)\n",
        "    epsilon = distances[:, -1]\n",
        "\n",
        "    # Count neighbors in x and y spaces within epsilon\n",
        "    n_x = np.zeros(n_samples)\n",
        "    n_y = np.zeros(n_samples)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Count points in x-space within epsilon\n",
        "        n_x[i] = np.sum(np.linalg.norm(x - x[i], axis=1) < epsilon[i])\n",
        "\n",
        "        # Count points in y-space within epsilon\n",
        "        n_y[i] = np.sum(np.linalg.norm(y - y[i], axis=1) < epsilon[i])\n",
        "\n",
        "    # Estimate mutual information using Kraskov's formula\n",
        "    psi_n = psi(n_samples)\n",
        "    psi_k = psi(k)\n",
        "\n",
        "    mi = psi_n + psi_k - np.mean(psi(n_x + 1) + psi(n_y + 1))\n",
        "\n",
        "    # Convert from nats to bits\n",
        "    mi_bits = mi / np.log(2)\n",
        "\n",
        "    return max(0, mi_bits)\n",
        "\n",
        "# Calculate mutual information between input and latent representations using the first seed\n",
        "mi_results = {}\n",
        "for dim in latent_dims:\n",
        "    encoder = autoencoders_seed0[dim][1]\n",
        "    latent_codes = encoder.predict(test_data_seed0, verbose=0)\n",
        "\n",
        "    # For each dimension, calculate MI with input\n",
        "    total_mi = 0\n",
        "    for i in range(latent_codes.shape[1]):  # For each latent dimension\n",
        "        latent_dim_i = latent_codes[:, i].reshape(-1, 1)\n",
        "        mi = knn_mi_estimation(test_data_seed0, latent_dim_i, k=5)\n",
        "        total_mi += mi\n",
        "\n",
        "    mi_results[dim] = total_mi\n",
        "    print(f\"Latent Dimension: {dim}, Mutual Information: {total_mi:.4f} bits\")\n",
        "\n",
        "# Plot information vs reconstruction error\n",
        "mi_values = [mi_results[dim] for dim in latent_dims]\n",
        "mse_values = [all_results[seeds[0]]['histories'][dim]['val_loss'][-1] for dim in latent_dims]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mi_values, mse_values, 'o-')\n",
        "for i, dim in enumerate(latent_dims):\n",
        "    plt.annotate(f'dim={dim}', (mi_values[i], mse_values[i]), xytext=(5, 5), textcoords='offset points')\n",
        "plt.xlabel('Mutual Information (bits)')\n",
        "plt.ylabel('Reconstruction Error (MSE)')\n",
        "plt.title('Information-Reconstruction Trade-off')\n",
        "plt.grid(True)\n",
        "plt.savefig('information_reconstruction_tradeoff.png')\n",
        "plt.close()\n",
        "\n",
        "###############################################################################\n",
        "# 8. Compare with VAE approach (theoretical comparison)\n",
        "###############################################################################\n",
        "print(\"\\n=== 8. Comparison with VAE Approach ===\")\n",
        "print(\"Our noise-based approach controls information flow by adding Gaussian noise\")\n",
        "print(\"to the latent representation, which is similar to the VAE's approach of\")\n",
        "print(\"modeling the latent distribution as Gaussian.\")\n",
        "print()\n",
        "print(\"Key differences:\")\n",
        "print(\"1. Our approach explicitly controls SNR, while VAE uses KL divergence\")\n",
        "print(\"2. VAE learns both mean and variance of the latent distribution\")\n",
        "print(\"3. Our MMD regularization encourages Gaussian latent distribution like VAE\")\n",
        "print(\"4. VAE's ELBO objective combines reconstruction and distribution matching\")\n",
        "print()\n",
        "print(\"Both approaches aim to create a well-structured latent space suitable\")\n",
        "print(\"for generation, but VAE provides a more principled probabilistic framework.\")\n",
        "\n",
        "###############################################################################\n",
        "# 10. Missing Visualization Plots\n",
        "###############################################################################\n",
        "print(\"\\n=== 10. Generating Missing Visualization Plots ===\")\n",
        "\n",
        "# 1. 3D Visualization of Original Data vs Reconstructions at Various SNRs and Dimensions\n",
        "def plot_3d_comparisons():\n",
        "    \"\"\"Create 3D plots comparing original data with reconstructions at various settings\"\"\"\n",
        "    # Use the first seed for consistency\n",
        "    seed_data = all_results[seeds[0]]\n",
        "\n",
        "    # Select a subset of data for visualization\n",
        "    sample_indices = np.random.choice(len(seed_data['test_data']), 500, replace=False)\n",
        "    original_samples = seed_data['test_data'][sample_indices]\n",
        "\n",
        "    # Create subplots for different dimensions and SNRs\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Settings to visualize\n",
        "    dims_to_plot = [1, 2, 4, 8]\n",
        "    snrs_to_plot = [-10, 0, 10, 30]\n",
        "\n",
        "    plot_idx = 1\n",
        "    for i, dim in enumerate(dims_to_plot):\n",
        "        encoder = seed_data['autoencoders'][dim][1]\n",
        "        decoder = seed_data['autoencoders'][dim][2]\n",
        "\n",
        "        for j, snr_db in enumerate(snrs_to_plot):\n",
        "            # Get reconstructions with noise\n",
        "            _, _, reconstructed = evaluate_reconstruction(encoder, decoder, original_samples, snr_db)\n",
        "\n",
        "            # Original data\n",
        "            ax = fig.add_subplot(len(dims_to_plot), len(snrs_to_plot)*2, plot_idx, projection='3d')\n",
        "            ax.scatter(original_samples[:, 0], original_samples[:, 1], original_samples[:, 2],\n",
        "                      alpha=0.5, s=10)\n",
        "            ax.set_title(f'Original Data\\n(Dim={dim}, SNR={snr_db}dB)')\n",
        "            ax.set_xlim(-1.5, 1.5)\n",
        "            ax.set_ylim(-1.5, 1.5)\n",
        "            ax.set_zlim(-1.5, 1.5)\n",
        "            plot_idx += 1\n",
        "\n",
        "            # Reconstructed data\n",
        "            ax = fig.add_subplot(len(dims_to_plot), len(snrs_to_plot)*2, plot_idx, projection='3d')\n",
        "            ax.scatter(reconstructed[:, 0], reconstructed[:, 1], reconstructed[:, 2],\n",
        "                      alpha=0.5, s=10, c='red')\n",
        "            ax.set_title(f'Reconstructed Data\\n(Dim={dim}, SNR={snr_db}dB)')\n",
        "            ax.set_xlim(-1.5, 1.5)\n",
        "            ax.set_ylim(-1.5, 1.5)\n",
        "            ax.set_zlim(-1.5, 1.5)\n",
        "            plot_idx += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('3d_reconstruction_comparisons.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# 2. Validation Loss Curves for Different Latent Dimensions\n",
        "def plot_validation_curves():\n",
        "    \"\"\"Plot validation loss curves for different latent dimensions\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Use the first seed for consistency\n",
        "    seed_data = all_results[seeds[0]]\n",
        "\n",
        "    for dim in latent_dims:\n",
        "        val_loss = seed_data['histories'][dim]['val_loss']\n",
        "        plt.plot(val_loss, label=f'Latent Dim = {dim}')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Loss (MSE)')\n",
        "    plt.title('Validation Loss Curves for Different Latent Dimensions')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.yscale('log')  # Use log scale for better visualization\n",
        "    plt.savefig('validation_loss_curves.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# 3. Information Rate vs Reconstruction Quality\n",
        "def plot_info_rate_vs_quality():\n",
        "    \"\"\"Plot information rate vs reconstruction quality for different dimensions\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Use the first seed for consistency\n",
        "    seed_data = all_results[seeds[0]]\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(latent_dims)))\n",
        "\n",
        "    for i, dim in enumerate(latent_dims):\n",
        "        mse_values = []\n",
        "        info_rates = []\n",
        "\n",
        "        for snr_db in snr_values:\n",
        "            mse = results[dim][snr_db]['mse']\n",
        "            info_rate = results[dim][snr_db]['info_rate']\n",
        "            mse_values.append(mse)\n",
        "            info_rates.append(info_rate)\n",
        "\n",
        "        plt.plot(info_rates, mse_values, 'o-', color=colors[i], label=f'Dim={dim}', markersize=6)\n",
        "\n",
        "        # Annotate SNR points\n",
        "        for j, snr_db in enumerate(snr_values):\n",
        "            if snr_db in [-10, 0, 10, 20, 30]:  # Only label some points to avoid clutter\n",
        "                plt.annotate(f'{snr_db}dB',\n",
        "                            (info_rates[j], mse_values[j]),\n",
        "                            xytext=(5, 5), textcoords='offset points',\n",
        "                            fontsize=8, alpha=0.7)\n",
        "\n",
        "    plt.xlabel('Information Rate (bits/sample)')\n",
        "    plt.ylabel('Reconstruction MSE')\n",
        "    plt.title('Information Rate vs Reconstruction Quality')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.yscale('log')\n",
        "    plt.savefig('info_rate_vs_quality.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# 4. Latent Space Visualization for Different Dimensions\n",
        "def plot_latent_spaces():\n",
        "    \"\"\"Visualize latent spaces for different dimensions\"\"\"\n",
        "    # Use the first seed for consistency\n",
        "    seed_data = all_results[seeds[0]]\n",
        "\n",
        "    # Create a grid of plots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, dim in enumerate(latent_dims[:6]):  # Only plot first 6 dimensions\n",
        "        encoder = seed_data['autoencoders'][dim][1]\n",
        "        latent_codes = encoder.predict(seed_data['test_data'], verbose=0)\n",
        "\n",
        "        if dim == 1:\n",
        "            # 1D histogram\n",
        "            axes[i].hist(latent_codes.flatten(), bins=50, alpha=0.7)\n",
        "            axes[i].set_title(f'Latent Dimension {dim}')\n",
        "            axes[i].set_xlabel('Latent Value')\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "        elif dim == 2:\n",
        "            # 2D scatter plot\n",
        "            axes[i].scatter(latent_codes[:, 0], latent_codes[:, 1], alpha=0.5, s=10)\n",
        "            axes[i].set_title(f'Latent Dimension {dim}')\n",
        "            axes[i].set_xlabel('Latent Dimension 1')\n",
        "            axes[i].set_ylabel('Latent Dimension 2')\n",
        "        else:\n",
        "            # For higher dimensions, show the first two dimensions\n",
        "            axes[i].scatter(latent_codes[:, 0], latent_codes[:, 1], alpha=0.5, s=10)\n",
        "            axes[i].set_title(f'Latent Dimension {dim} (First 2 dims)')\n",
        "            axes[i].set_xlabel('Latent Dimension 1')\n",
        "            axes[i].set_ylabel('Latent Dimension 2')\n",
        "\n",
        "    # Remove empty subplots\n",
        "    for i in range(len(latent_dims), 6):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('latent_space_visualizations.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# 5. Training History for Different Latent Dimensions\n",
        "def plot_training_history():\n",
        "    \"\"\"Plot training history for different latent dimensions\"\"\"\n",
        "    # Use the first seed for consistency\n",
        "    seed_data = all_results[seeds[0]]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    metrics = ['loss', 'reconstruction_loss', 'mmd_loss', 'val_loss']\n",
        "    titles = ['Total Loss', 'Reconstruction Loss', 'MMD Loss', 'Validation Loss']\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        for dim in latent_dims:\n",
        "            history = seed_data['histories'][dim][metric]\n",
        "            axes[i].plot(history, label=f'Dim={dim}')\n",
        "\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel('Epoch')\n",
        "        axes[i].set_ylabel('Loss')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True)\n",
        "        if metric != 'mmd_loss':  # MMD loss is typically much smaller\n",
        "            axes[i].set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Generate all the missing plots\n",
        "print(\"Generating 3D comparison plots...\")\n",
        "plot_3d_comparisons()\n",
        "\n",
        "print(\"Generating validation loss curves...\")\n",
        "plot_validation_curves()\n",
        "\n",
        "print(\"Generating information rate vs quality plot...\")\n",
        "plot_info_rate_vs_quality()\n",
        "\n",
        "print(\"Generating latent space visualizations...\")\n",
        "plot_latent_spaces()\n",
        "\n",
        "print(\"Generating training history plots...\")\n",
        "plot_training_history()\n",
        "\n",
        "print(\"All missing visualizations have been generated and saved as PNG files.\")\n",
        "\n",
        "###############################################################################\n",
        "# 9. Summary of Results\n",
        "###############################################################################\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY OF RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n1. Autoencoder Performance by Latent Dimension (Mean ± Std across seeds):\")\n",
        "print(\"Dim\\tFinal MSE\\tMutual Info (bits)\")\n",
        "for dim in latent_dims:\n",
        "    val_losses = [all_results[seed]['histories'][dim]['val_loss'][-1] for seed in seeds]\n",
        "    final_mse = np.mean(val_losses)\n",
        "    mi = mi_results[dim]\n",
        "    print(f\"{dim}\\t{final_mse:.6f} ± {np.std(val_losses):.6f}\\t{mi:.4f}\")\n",
        "\n",
        "print(\"\\n2. Information Rate at Different SNRs (for dim=2):\")\n",
        "for snr_db in snr_values:\n",
        "    _, info_rate, _, _ = add_latent_noise(encoder, test_data_seed0, snr_db)\n",
        "    print(f\"SNR: {snr_db} dB -> Info Rate: {info_rate:.4f} bits/sample\")\n",
        "\n",
        "print(\"\\n3. Generative Performance:\")\n",
        "print(\"Dim\\tSurface %\\tKL Divergence\")\n",
        "for dim in latent_dims:\n",
        "    perf = generative_performance[dim]\n",
        "    print(f\"{dim}\\t{perf['surface_percentage']:.2f}%\\t\\t{perf['kl_divergence']:.4f}\")\n",
        "\n",
        "print(\"\\n4. MMD Sigma Ablation Results:\")\n",
        "print(\"Sigma\\tVal Loss\\tSurface %\\tKL Divergence\")\n",
        "for sigma in mmd_sigma_values:\n",
        "    result = sigma_results[sigma]\n",
        "    print(f\"{sigma}\\t{result['final_val_loss']:.6f}\\t{result['surface_percentage']:.2f}\\t\\t{result['kl_divergence']:.4f}\")\n",
        "\n",
        "print(\"\\n5. Surface Tolerance Ablation Results:\")\n",
        "print(\"Tolerance\\tSurface %\\tKL Divergence\")\n",
        "for tolerance in tolerance_values:\n",
        "    result = tolerance_results[tolerance]\n",
        "    print(f\"{tolerance}\\t\\t{result['surface_percentage']:.2f}\\t\\t{result['kl_divergence']:.4f}\")\n",
        "\n",
        "print(\"\\n6. Key Observations:\")\n",
        "print(\"- Higher latent dimensions allow more information preservation but may overfit\")\n",
        "print(\"- Very low SNRs (<0 dB) destroy most information and yield poor reconstructions\")\n",
        "print(\"- The optimal latent dimension for generation appears to be around 2-4\")\n",
        "print(\"- The autoencoder learns to capture the cube surface structure reasonably well\")\n",
        "print(\"- Information rate increases with SNR as expected from theory\")\n",
        "print(\"- MMD regularization helps structure the latent space for better generation\")\n",
        "print(\"- MMD sigma affects both reconstruction quality and generative performance\")\n",
        "print(\"- Surface tolerance parameter affects the evaluation of generative quality\")\n",
        "\n",
        "# Save results to file\n",
        "with open('results_summary.txt', 'w') as f:\n",
        "    f.write(\"RESULTS SUMMARY\\n\")\n",
        "    f.write(\"===============\\n\\n\")\n",
        "\n",
        "    f.write(\"Autoencoder Performance by Latent Dimension (Mean ± Std across seeds):\\n\")\n",
        "    f.write(\"Dim\\tFinal MSE\\tMutual Info (bits)\\n\")\n",
        "    for dim in latent_dims:\n",
        "        val_losses = [all_results[seed]['histories'][dim]['val_loss'][-1] for seed in seeds]\n",
        "        final_mse = np.mean(val_losses)\n",
        "        mi = mi_results[dim]\n",
        "        f.write(f\"{dim}\\t{final_mse:.6f} ± {np.std(val_losses):.6f}\\t{mi:.4f}\\n\")\n",
        "\n",
        "    f.write(\"\\nInformation Rate at Different SNRs (for dim=2):\\n\")\n",
        "    for snr_db in snr_values:\n",
        "        _, info_rate, _, _ = add_latent_noise(encoder, test_data_seed0, snr_db)\n",
        "        f.write(f\"SNR: {snr_db} dB -> Info Rate: {info_rate:.4f} bits/sample\\n\")\n",
        "\n",
        "    f.write(\"\\nGenerative Performance:\\n\")\n",
        "    f.write(\"Dim\\tSurface %\\tKL Divergence\\n\")\n",
        "    for dim in latent_dims:\n",
        "        perf = generative_performance[dim]\n",
        "        f.write(f\"{dim}\\t{perf['surface_percentage']:.2f}%\\t\\t{perf['kl_divergence']:.4f}\\n\")\n",
        "\n",
        "    f.write(\"\\nMMD Sigma Ablation Results:\\n\")\n",
        "    f.write(\"Sigma\\tVal Loss\\tSurface %\\tKL Divergence\\n\")\n",
        "    for sigma in mmd_sigma_values:\n",
        "        result = sigma_results[sigma]\n",
        "        f.write(f\"{sigma}\\t{result['final_val_loss']:.6f}\\t{result['surface_percentage']:.2f}\\t\\t{result['kl_divergence']:.4f}\\n\")\n",
        "\n",
        "    f.write(\"\\nSurface Tolerance Ablation Results:\\n\")\n",
        "    f.write(\"Tolerance\\tSurface %\\tKL Divergence\\n\")\n",
        "    for tolerance in tolerance_values:\n",
        "        result = tolerance_results[tolerance]\n",
        "        f.write(f\"{tolerance}\\t\\t{result['surface_percentage']:.2f}\\t\\t{result['kl_divergence']:.4f}\\n\")\n",
        "\n",
        "print(\"\\nResults saved to 'results_summary.txt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48392a3d"
      },
      "source": [
        "# Autoencoder Assignment 1: Information-Theoretic Autoencoder\n",
        "\n",
        "This repository contains the implementation and analysis of an autoencoder applied to data uniformly distributed on the surface of a 3D cube. The project explores the trade-off between reconstruction quality and information preservation in the latent space, incorporating Maximum Mean Discrepancy (MMD) regularization and analyzing mutual information using a k-NN based method.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "The code is structured to follow the assignment requirements step-by-step:\n",
        "\n",
        "1.  **Data Generation**: Creates 3D data points uniformly distributed on the surface of a cube.\n",
        "2.  **Autoencoder Implementation**: Builds an autoencoder model with a configurable latent dimension and incorporates MMD regularization to shape the latent distribution.\n",
        "3.  **Probabilistic Interpretation**: Discusses the probabilistic interpretation of the Mean Squared Error (MSE) objective function.\n",
        "4.  **Latent Space Control and Noise**: Implements a method to add Gaussian noise to the latent representation based on Signal-to-Noise Ratio (SNR) and calculates the theoretical information rate.\n",
        "5.  **Reconstruction Analysis**: Evaluates the autoencoder's reconstruction performance at various latent dimensions and SNRs.\n",
        "6.  **Generative System**: Creates a generative model by fitting a Gaussian distribution to the learned latent space and evaluates the quality of generated samples.\n",
        "7.  **Mutual Information Estimation**: Uses a k-NN based method to estimate the mutual information between the input data and the latent representation.\n",
        "8.  **VAE Comparison**: Provides a theoretical comparison of the implemented approach with Variational Autoencoders (VAEs).\n",
        "9.  **Results Summary**: Summarizes the key findings and observations from the experiments.\n",
        "\n",
        "## Results Summary\n",
        "\n",
        "Based on the experiments conducted with different latent dimensions and SNRs, the following key results and observations were made:\n",
        "\n",
        "| Latent Dimension | Final MSE  | Mutual Info (bits) | Surface % | KL Divergence |\n",
        "|------------------|------------|--------------------|-----------|---------------|\n",
        "| 1                | 0.099204   | 2.2934             | 10.60%    | 2.8686        |\n",
        "| 2                | 0.000817   | 4.6710             | 98.50%    | 0.1485        |\n",
        "| 4                | 0.000274   | 7.5805             | 20.90%    | 0.1698        |\n",
        "| 8                | 0.000116   | 16.2127            | 12.50%    | 0.2286        |\n",
        "| 16               | 0.000103   | 32.9915            | 21.70%    | 0.1406        |\n",
        "\n",
        "Information Rate at Different SNRs (for dim=2):\n",
        "\n",
        "| SNR (dB) | Info Rate (bits/sample) |\n",
        "|----------|-------------------------|\n",
        "| -10      | 0.0688                  |\n",
        "| -5       | 0.1982                  |\n",
        "| 0        | 0.5000                  |\n",
        "| 5        | 1.0287                  |\n",
        "| 10       | 1.7297                  |\n",
        "| 20       | 3.3291                  |\n",
        "| 30       | 4.9836                  |\n",
        "\n",
        "Key Observations:\n",
        "\n",
        "*   Higher latent dimensions generally lead to lower reconstruction error but may capture more noise or irrelevant information, potentially hindering generative performance.\n",
        "*   Very low SNRs significantly degrade reconstruction quality due to excessive noise in the latent space.\n",
        "*   The optimal latent dimension for generating samples that resemble the original cube surface appears to be around 2-4, balancing reconstruction quality and latent space structure.\n",
        "*   The autoencoder effectively learns to represent the cube surface structure in the latent space.\n",
        "*   The information rate between the input and noisy latent representation increases with SNR, consistent with information theory.\n",
        "*   MMD regularization helps to shape the latent distribution towards a standard Gaussian, which is beneficial for the generative process.\n",
        "\n",
        "## Files\n",
        "\n",
        "*   `autoencoder_assignment.ipynb`: The main notebook containing the code implementation and analysis.\n",
        "*   `cube_surface_data.png`: Visualization of the generated 3D cube surface data.\n",
        "*   `training_history_dim_*.png`: Plots showing the training history (loss curves) for autoencoders with different latent dimensions.\n",
        "*   `reconstruction_comparison.png`: Visualization comparing original test data, reconstructed data, and the distribution of reconstruction errors for a specific latent dimension and SNR.\n",
        "*   `generated_samples.png`: Visualization of samples generated by the best-performing generative model.\n",
        "*   `information_reconstruction_tradeoff.png`: Plot showing the relationship between mutual information and reconstruction error for different latent dimensions.\n",
        "*   `results_summary.txt`: A text file containing the summarized results.\n",
        "\n",
        "## How to Run\n",
        "\n",
        "1.  Open the `autoencoder_assignment.ipynb` notebook in Google Colab or a similar environment.\n",
        "2.  Run all the cells sequentially.\n",
        "3.  The code will generate the data, train autoencoders with different latent dimensions, perform analysis, and save plots and a results summary file.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "The code requires the following libraries:\n",
        "\n",
        "*   `numpy`\n",
        "*   `tensorflow`\n",
        "*   `matplotlib`\n",
        "*   `sklearn`\n",
        "*   `scipy`\n",
        "*   `seaborn`\n",
        "\n",
        "These can be installed using pip:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8af8d60"
      },
      "source": [
        "# Installation Instructions\n",
        "\n",
        "To run the code in this notebook, you need to install the following libraries:\n",
        "\n",
        "*   `numpy`\n",
        "*   `tensorflow`\n",
        "*   `matplotlib`\n",
        "*   `sklearn`\n",
        "*   `scipy`\n",
        "*   `seaborn`\n",
        "\n",
        "These can be installed using pip:"
      ]
    }
  ]
}